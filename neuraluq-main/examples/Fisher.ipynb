{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dc0411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuraluq as neuq\n",
    "import neuraluq.variables as neuq_vars\n",
    "from neuraluq.config import tf\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd8c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(noise_u, noise_f):\n",
    "    data = sio.loadmat(\"../dataset/Fisher.mat\")\n",
    "    x_u_train, t_u_train = data[\"x_u_train\"], data[\"t_u_train\"]\n",
    "    x_f_train, t_f_train = data[\"x_f_train\"], data[\"t_f_train\"]\n",
    "    x_test, t_test, u_test = data[\"x_test\"], data[\"t_test\"], data[\"u_test\"]\n",
    "    x_test, t_test, u_test = (\n",
    "        x_test.reshape([-1, 1]),\n",
    "        t_test.reshape([-1, 1]),\n",
    "        u_test.reshape([-1, 1]),\n",
    "    )\n",
    "    u_train, f_train = data[\"u_train\"], data[\"f_train\"]\n",
    "    train_u = x_u_train, t_u_train, u_train\n",
    "    train_f = x_f_train, t_f_train, f_train\n",
    "    test = x_test, t_test, u_test\n",
    "    return train_u, train_f, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "447a2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pde_fn(x, u, r):\n",
    "    d = 1\n",
    "    \n",
    "    u_x, u_t = tf.split(tf.gradients(u, x)[0], 2, axis=-1)\n",
    "    u_xx = tf.gradients(u_x, x)[0][..., 0:1]\n",
    "    \n",
    "    f = u_t - r * u * (1 - u) - d * u_xx\n",
    "    #f = u_t - r * u_xx\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a6076c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@neuq.utils.timer\n",
    "def Samplable(\n",
    "    x_u_train, t_u_train, u_train, x_f_train, t_f_train, f_train, noise, layers\n",
    "):\n",
    "    # build processes\n",
    "    process_u = neuq.process.Process(\n",
    "        surrogate=neuq.surrogates.FNN(layers=layers),\n",
    "        prior=neuq_vars.fnn.Samplable(layers=layers, mean=0, sigma=1),\n",
    "    )\n",
    "    process_logk_1 = neuq.process.Process(\n",
    "        surrogate=neuq.surrogates.Identity(),\n",
    "        prior=neuq_vars.const.Samplable(mean=1, sigma=1),\n",
    "    )\n",
    "    \n",
    "    # build likelihood\n",
    "    likelihood_u = neuq.likelihoods.Normal(\n",
    "        inputs=np.concatenate([x_u_train, t_u_train], axis=-1),\n",
    "        targets=u_train,\n",
    "        processes=[process_u],\n",
    "        sigma=noise,\n",
    "    )\n",
    "    likelihood_f = neuq.likelihoods.Normal(\n",
    "        inputs=np.concatenate([x_f_train, t_f_train], axis=-1),\n",
    "        targets=f_train,\n",
    "        processes=[process_u, process_logk_1],\n",
    "        pde=pde_fn,\n",
    "        sigma=noise,\n",
    "    )\n",
    "    # build model\n",
    "    model = neuq.models.Model(\n",
    "        processes=[process_u, process_logk_1],\n",
    "        likelihoods=[likelihood_u, likelihood_f],\n",
    "    )\n",
    "    # assign and compile method\n",
    "    # Change the parameters to make the acceptance rate close to 0.6.\n",
    "    method = neuq.inferences.HMC(\n",
    "        num_samples=500,\n",
    "        num_burnin=3000,\n",
    "        init_time_step=0.01,\n",
    "        leapfrog_step=50,\n",
    "        seed=66,\n",
    "    )\n",
    "    model.compile(method)\n",
    "    # obtain posterior samples\n",
    "    samples, results = model.run()\n",
    "    print(\"Acceptance rate: %.3f \\n\"%(np.mean(results)))\n",
    "\n",
    "    processes = [process_u, process_logk_1]\n",
    "    return processes, samples, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c52501",
   "metadata": {},
   "outputs": [],
   "source": [
    "@neuq.utils.timer\n",
    "def Trainable(\n",
    "    x_u_train, t_u_train, u_train, x_f_train, t_f_train, f_train, noise, layers\n",
    "):\n",
    "    # build processes\n",
    "    process_u = neuq.process.Process(\n",
    "        surrogate=neuq.surrogates.FNN(layers=layers),\n",
    "        posterior=neuq_vars.fnn.Trainable(layers=layers),\n",
    "    )\n",
    "    process_logk_1 = neuq.process.Process(\n",
    "        surrogate=neuq.surrogates.Identity(),\n",
    "        posterior=neuq_vars.const.Trainable(value=1),\n",
    "    )\n",
    "    \n",
    "    loss_u = neuq.likelihoods.MSE(\n",
    "        inputs=np.concatenate([x_u_train, t_u_train], axis=-1),\n",
    "        targets=u_train,\n",
    "        processes=[process_u],\n",
    "        multiplier=1.0,\n",
    "    )\n",
    "    loss_f = neuq.likelihoods.MSE(\n",
    "        inputs=np.concatenate([x_f_train, t_f_train], axis=-1),\n",
    "        targets=f_train,\n",
    "        processes=[process_u, process_logk_1],\n",
    "        pde=pde_fn,\n",
    "        multiplier=1.0,\n",
    "    )\n",
    "    # build model\n",
    "    model = neuq.models.Model(\n",
    "        processes=[process_u, process_logk_1],\n",
    "        likelihoods=[loss_u, loss_f],\n",
    "    )\n",
    "    # assign and compile method\n",
    "    method = neuq.inferences.DEns(\n",
    "        num_samples=20, num_iterations=20000, optimizer=tf.train.AdamOptimizer(1e-3),\n",
    "    )\n",
    "    model.compile(method)\n",
    "    # obtain posterior samples\n",
    "    samples = model.run()\n",
    "    samples = neuq.utils.batch_samples(samples)  # reshape\n",
    "\n",
    "    processes = [process_u, process_logk_1]\n",
    "    return processes, samples, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de00c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots(\n",
    "    logk_1_pred,\n",
    "    u_pred,\n",
    "    x_test,\n",
    "    t_test,\n",
    "    u_test,\n",
    "    x_u_train,\n",
    "    t_u_train,\n",
    "    u_train,\n",
    "):\n",
    "    ### DA CAPIRE LA STORIA DEL PERCHè PRENDE L'ESPONENZIALE DELLE VARIABILI\n",
    "    \n",
    "    #k_1_pred = np.exp(logk_1_pred)\n",
    "    k_1_pred = logk_1_pred\n",
    "    print(\"Mean & Std of k1 are %.3f, %.3f\" % (np.mean(k_1_pred), np.std(k_1_pred)))\n",
    "    \n",
    "    u_pred = np.reshape(u_pred, [-1, NT, NX])\n",
    "    mu = np.mean(u_pred, axis=0)\n",
    "    std = np.std(u_pred, axis=0)\n",
    "    \n",
    "    x_test = np.reshape(x_test, [NT, NX])\n",
    "    t_test = np.reshape(t_test, [NT, NX])\n",
    "    u_test = np.reshape(u_test, [NT, NX])\n",
    "    \n",
    "    # cambiare i per avere plot su altri istanti di tempo\n",
    "    i = 0\n",
    "    \n",
    "    current_t = t_test[i][0]\n",
    "    # current_x*10 PERCHè PRIMA LA X è STATA NORMALIZZATA\n",
    "    current_x = x_u_train[t_u_train == current_t]\n",
    "    current_u = u_train[t_u_train == current_t]\n",
    "    # std = np.sqrt(std**2 + 0.1**2)\n",
    "    plt.plot(np.linspace(0, 3, NX), mu[i, :], \"--\", label=\"mean\")\n",
    "    plt.fill_between(\n",
    "        np.linspace(0, 3, NX), (mu + 2 * std)[i, :], (mu - 2 * std)[i, :], alpha=0.3\n",
    "    )\n",
    "    plt.plot(np.linspace(0, 3, NX), u_test[i, :], label=\"reference\")\n",
    "    plt.plot(current_x, current_u, \"o\", label=\"observations\")\n",
    "    plt.legend()\n",
    "    plt.title(\"t=\" + str(current_t))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "881d9331",
   "metadata": {},
   "outputs": [],
   "source": [
    "NT, NX = 60, 300\n",
    "noise = 0.1\n",
    "train_u, train_f, test = load_data(noise, noise)\n",
    "x_u_train, t_u_train, u_train = train_u\n",
    "x_f_train, t_f_train, f_train = train_f\n",
    "x_test, t_test, u_test = test\n",
    "\n",
    "layers = [2, 50, 50, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "205e1aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supporting backend tensorflow.compat.v1\n",
      "\n",
      "Compiling a MCMC method\n",
      "\n",
      "sampling from posterior distribution ...\n",
      "\n",
      "Finished sampling from posterior distribution ...\n",
      "\n",
      "Acceptance rate: 0.144 \n",
      "\n",
      "Execution time for 'Samplable' function is: 466.486 s, 7.775 mins\n"
     ]
    }
   ],
   "source": [
    "processes, samples, model = Samplable(x_u_train, t_u_train, u_train, x_f_train, t_f_train, f_train, noise, layers,)\n",
    "\n",
    "#processes, samples, model = Trainable(x_u_train, t_u_train, u_train, x_f_train, t_f_train, f_train, noise, layers,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "145f6e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean & Std of k1 are -1.579, 0.030\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred, logk_1_pred = model.predict(np.concatenate([x_test, t_test], axis=-1), samples, processes, pde_fn=None,)\n",
    "\n",
    "plots(\n",
    "    logk_1_pred,\n",
    "    u_pred,\n",
    "    x_test,\n",
    "    t_test,\n",
    "    u_test,\n",
    "    x_u_train,\n",
    "    t_u_train,\n",
    "    u_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc7a6e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supporting backend tensorflow.compat.v1\n",
      "\n",
      "Compiling a Ensemble method\n",
      "\n",
      "Generating 0th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.066235766\n",
      "Iteration:  100 , loss:  0.03859888\n",
      "Iteration:  200 , loss:  0.036128078\n",
      "Iteration:  300 , loss:  0.03235684\n",
      "Iteration:  400 , loss:  0.02865601\n",
      "Iteration:  500 , loss:  0.026626214\n",
      "Iteration:  600 , loss:  0.025195198\n",
      "Iteration:  700 , loss:  0.023881834\n",
      "Iteration:  800 , loss:  0.022728989\n",
      "Iteration:  900 , loss:  0.021261942\n",
      "Iteration:  1000 , loss:  0.020288724\n",
      "Iteration:  1100 , loss:  0.019437147\n",
      "Iteration:  1200 , loss:  0.01867753\n",
      "Iteration:  1300 , loss:  0.017960941\n",
      "Iteration:  1400 , loss:  0.01729114\n",
      "Iteration:  1500 , loss:  0.016637586\n",
      "Iteration:  1600 , loss:  0.01602023\n",
      "Iteration:  1700 , loss:  0.0154639045\n",
      "Iteration:  1800 , loss:  0.014996538\n",
      "Iteration:  1900 , loss:  0.01454929\n",
      "Iteration:  2000 , loss:  0.014120467\n",
      "Iteration:  2100 , loss:  0.013635539\n",
      "Iteration:  2200 , loss:  0.015804136\n",
      "Iteration:  2300 , loss:  0.012525521\n",
      "Iteration:  2400 , loss:  0.01206292\n",
      "Iteration:  2500 , loss:  0.011691618\n",
      "Iteration:  2600 , loss:  0.01134609\n",
      "Iteration:  2700 , loss:  0.011025129\n",
      "Iteration:  2800 , loss:  0.010717356\n",
      "Iteration:  2900 , loss:  0.011419858\n",
      "Iteration:  3000 , loss:  0.010142063\n",
      "Iteration:  3100 , loss:  0.00986949\n",
      "Iteration:  3200 , loss:  0.009697727\n",
      "Iteration:  3300 , loss:  0.009397596\n",
      "Iteration:  3400 , loss:  0.009184423\n",
      "Iteration:  3500 , loss:  0.009012084\n",
      "Iteration:  3600 , loss:  0.008851535\n",
      "Iteration:  3700 , loss:  0.009935679\n",
      "Iteration:  3800 , loss:  0.008598142\n",
      "Iteration:  3900 , loss:  0.008493794\n",
      "Iteration:  4000 , loss:  0.008413999\n",
      "Iteration:  4100 , loss:  0.008333162\n",
      "Iteration:  4200 , loss:  0.008367103\n",
      "Iteration:  4300 , loss:  0.008206591\n",
      "Iteration:  4400 , loss:  0.008149283\n",
      "Iteration:  4500 , loss:  0.00810394\n",
      "Iteration:  4600 , loss:  0.00805698\n",
      "Iteration:  4700 , loss:  0.008012345\n",
      "Iteration:  4800 , loss:  0.007975895\n",
      "Iteration:  4900 , loss:  0.0079379855\n",
      "Iteration:  5000 , loss:  0.00823774\n",
      "Iteration:  5100 , loss:  0.007871877\n",
      "Iteration:  5200 , loss:  0.007840054\n",
      "Iteration:  5300 , loss:  0.007820953\n",
      "Iteration:  5400 , loss:  0.0077856993\n",
      "Iteration:  5500 , loss:  0.0077592647\n",
      "Iteration:  5600 , loss:  0.007737859\n",
      "Iteration:  5700 , loss:  0.007714172\n",
      "Iteration:  5800 , loss:  0.0081647895\n",
      "Iteration:  5900 , loss:  0.0076747457\n",
      "Iteration:  6000 , loss:  0.0076556536\n",
      "Iteration:  6100 , loss:  0.0077439873\n",
      "Iteration:  6200 , loss:  0.007622947\n",
      "Iteration:  6300 , loss:  0.0076066614\n",
      "Iteration:  6400 , loss:  0.0075940844\n",
      "Iteration:  6500 , loss:  0.0075792414\n",
      "Iteration:  6600 , loss:  0.007565296\n",
      "Iteration:  6700 , loss:  0.007554776\n",
      "Iteration:  6800 , loss:  0.007541701\n",
      "Iteration:  6900 , loss:  0.007590927\n",
      "Iteration:  7000 , loss:  0.007520094\n",
      "Iteration:  7100 , loss:  0.0075087305\n",
      "Iteration:  7200 , loss:  0.007498095\n",
      "Iteration:  7300 , loss:  0.0074889255\n",
      "Iteration:  7400 , loss:  0.007478564\n",
      "Iteration:  7500 , loss:  0.0074712127\n",
      "Iteration:  7600 , loss:  0.007460397\n",
      "Iteration:  7700 , loss:  0.007450774\n",
      "Iteration:  7800 , loss:  0.0074523645\n",
      "Iteration:  7900 , loss:  0.007433284\n",
      "Iteration:  8000 , loss:  0.007423997\n",
      "Iteration:  8100 , loss:  0.0074164113\n",
      "Iteration:  8200 , loss:  0.007406869\n",
      "Iteration:  8300 , loss:  0.0091576455\n",
      "Iteration:  8400 , loss:  0.007389622\n",
      "Iteration:  8500 , loss:  0.0073803263\n",
      "Iteration:  8600 , loss:  0.0073759896\n",
      "Iteration:  8700 , loss:  0.0073637203\n",
      "Iteration:  8800 , loss:  0.0073804045\n",
      "Iteration:  8900 , loss:  0.0073468518\n",
      "Iteration:  9000 , loss:  0.0073377974\n",
      "Iteration:  9100 , loss:  0.0073440876\n",
      "Iteration:  9200 , loss:  0.0073214746\n",
      "Iteration:  9300 , loss:  0.007792721\n",
      "Iteration:  9400 , loss:  0.007305659\n",
      "Iteration:  9500 , loss:  0.007297042\n",
      "Iteration:  9600 , loss:  0.0072915973\n",
      "Iteration:  9700 , loss:  0.007281623\n",
      "Iteration:  9800 , loss:  0.0077421474\n",
      "Iteration:  9900 , loss:  0.0072674626\n",
      "Iteration:  10000 , loss:  0.007259753\n",
      "Iteration:  10100 , loss:  0.009075087\n",
      "Iteration:  10200 , loss:  0.0072465898\n",
      "Iteration:  10300 , loss:  0.00723957\n",
      "Iteration:  10400 , loss:  0.0072348616\n",
      "Iteration:  10500 , loss:  0.007227886\n",
      "Iteration:  10600 , loss:  0.008721636\n",
      "Iteration:  10700 , loss:  0.007217409\n",
      "Iteration:  10800 , loss:  0.007211531\n",
      "Iteration:  10900 , loss:  0.007944415\n",
      "Iteration:  11000 , loss:  0.0072015617\n",
      "Iteration:  11100 , loss:  0.007195977\n",
      "Iteration:  11200 , loss:  0.007199507\n",
      "Iteration:  11300 , loss:  0.007186945\n",
      "Iteration:  11400 , loss:  0.0071817487\n",
      "Iteration:  11500 , loss:  0.007178382\n",
      "Iteration:  11600 , loss:  0.0071732933\n",
      "Iteration:  11700 , loss:  0.009505032\n",
      "Iteration:  11800 , loss:  0.0071652364\n",
      "Iteration:  11900 , loss:  0.007160566\n",
      "Iteration:  12000 , loss:  0.008076\n",
      "Iteration:  12100 , loss:  0.0071537374\n",
      "Iteration:  12200 , loss:  0.0071495324\n",
      "Iteration:  12300 , loss:  0.0071453424\n",
      "Iteration:  12400 , loss:  0.007143553\n",
      "Iteration:  12500 , loss:  0.0071383044\n",
      "Iteration:  12600 , loss:  0.0073417085\n",
      "Iteration:  12700 , loss:  0.0071315654\n",
      "Iteration:  12800 , loss:  0.0071277395\n",
      "Iteration:  12900 , loss:  0.0071266703\n",
      "Iteration:  13000 , loss:  0.007121391\n",
      "Iteration:  13100 , loss:  0.007211435\n",
      "Iteration:  13200 , loss:  0.0071152626\n",
      "Iteration:  13300 , loss:  0.007111664\n",
      "Iteration:  13400 , loss:  0.007109588\n",
      "Iteration:  13500 , loss:  0.00710561\n",
      "Iteration:  13600 , loss:  0.0071030166\n",
      "Iteration:  13700 , loss:  0.007099521\n",
      "Iteration:  13800 , loss:  0.0070998655\n",
      "Iteration:  13900 , loss:  0.007093762\n",
      "Iteration:  14000 , loss:  0.0071165576\n",
      "Iteration:  14100 , loss:  0.00708817\n",
      "Iteration:  14200 , loss:  0.0070855636\n",
      "Iteration:  14300 , loss:  0.007083247\n",
      "Iteration:  14400 , loss:  0.0070797554\n",
      "Iteration:  14500 , loss:  0.0071955803\n",
      "Iteration:  14600 , loss:  0.007074574\n",
      "Iteration:  14700 , loss:  0.007071496\n",
      "Iteration:  14800 , loss:  0.007069032\n",
      "Iteration:  14900 , loss:  0.007077704\n",
      "Iteration:  15000 , loss:  0.0070636445\n",
      "Iteration:  15100 , loss:  0.0070615914\n",
      "Iteration:  15200 , loss:  0.007058542\n",
      "Iteration:  15300 , loss:  0.0070578144\n",
      "Iteration:  15400 , loss:  0.007053592\n",
      "Iteration:  15500 , loss:  0.0070525096\n",
      "Iteration:  15600 , loss:  0.0071126083\n",
      "Iteration:  15700 , loss:  0.007046455\n",
      "Iteration:  15800 , loss:  0.0070529724\n",
      "Iteration:  15900 , loss:  0.0070418785\n",
      "Iteration:  16000 , loss:  0.0070792874\n",
      "Iteration:  16100 , loss:  0.0070374464\n",
      "Iteration:  16200 , loss:  0.0070943963\n",
      "Iteration:  16300 , loss:  0.007939808\n",
      "Iteration:  16400 , loss:  0.0070309364\n",
      "Iteration:  16500 , loss:  0.00703286\n",
      "Iteration:  16600 , loss:  0.0070270356\n",
      "Iteration:  16700 , loss:  0.0070255394\n",
      "Iteration:  16800 , loss:  0.00702398\n",
      "Iteration:  16900 , loss:  0.007021242\n",
      "Iteration:  17000 , loss:  0.007020043\n",
      "Iteration:  17100 , loss:  0.0070201354\n",
      "Iteration:  17200 , loss:  0.0071848277\n",
      "Iteration:  17300 , loss:  0.0070137572\n",
      "Iteration:  17400 , loss:  0.007011603\n",
      "Iteration:  17500 , loss:  0.007031273\n",
      "Iteration:  17600 , loss:  0.007008268\n",
      "Iteration:  17700 , loss:  0.0071005193\n",
      "Iteration:  17800 , loss:  0.0070050354\n",
      "Iteration:  17900 , loss:  0.007813293\n",
      "Iteration:  18000 , loss:  0.0070018624\n",
      "Iteration:  18100 , loss:  0.007001253\n",
      "Iteration:  18200 , loss:  0.006998713\n",
      "Iteration:  18300 , loss:  0.007039079\n",
      "Iteration:  18400 , loss:  0.0069953026\n",
      "Iteration:  18500 , loss:  0.0069957003\n",
      "Iteration:  18600 , loss:  0.0069922735\n",
      "Iteration:  18700 , loss:  0.0069913473\n",
      "Iteration:  18800 , loss:  0.0070011807\n",
      "Iteration:  18900 , loss:  0.0069930064\n",
      "Iteration:  19000 , loss:  0.006989116\n",
      "Iteration:  19100 , loss:  0.006986548\n",
      "Iteration:  19200 , loss:  0.006985361\n",
      "Iteration:  19300 , loss:  0.0069872662\n",
      "Iteration:  19400 , loss:  0.0070262756\n",
      "Iteration:  19500 , loss:  0.00698244\n",
      "Iteration:  19600 , loss:  0.0075537693\n",
      "Iteration:  19700 , loss:  0.006977548\n",
      "Iteration:  19800 , loss:  0.0077113146\n",
      "Iteration:  19900 , loss:  0.006975181\n",
      "Generating 1th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.06311652\n",
      "Iteration:  100 , loss:  0.03922575\n",
      "Iteration:  200 , loss:  0.036877714\n",
      "Iteration:  300 , loss:  0.033552606\n",
      "Iteration:  400 , loss:  0.029977556\n",
      "Iteration:  500 , loss:  0.027932687\n",
      "Iteration:  600 , loss:  0.02599965\n",
      "Iteration:  700 , loss:  0.02405358\n",
      "Iteration:  800 , loss:  0.02208091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  900 , loss:  0.020642255\n",
      "Iteration:  1000 , loss:  0.019433914\n",
      "Iteration:  1100 , loss:  0.018499948\n",
      "Iteration:  1200 , loss:  0.017902855\n",
      "Iteration:  1300 , loss:  0.01670367\n",
      "Iteration:  1400 , loss:  0.015863078\n",
      "Iteration:  1500 , loss:  0.015045667\n",
      "Iteration:  1600 , loss:  0.014283802\n",
      "Iteration:  1700 , loss:  0.013690218\n",
      "Iteration:  1800 , loss:  0.012917433\n",
      "Iteration:  1900 , loss:  0.012318909\n",
      "Iteration:  2000 , loss:  0.011754541\n",
      "Iteration:  2100 , loss:  0.011309325\n",
      "Iteration:  2200 , loss:  0.010912873\n",
      "Iteration:  2300 , loss:  0.010886538\n",
      "Iteration:  2400 , loss:  0.010226715\n",
      "Iteration:  2500 , loss:  0.009929646\n",
      "Iteration:  2600 , loss:  0.009675035\n",
      "Iteration:  2700 , loss:  0.009437968\n",
      "Iteration:  2800 , loss:  0.009447981\n",
      "Iteration:  2900 , loss:  0.009039122\n",
      "Iteration:  3000 , loss:  0.008869734\n",
      "Iteration:  3100 , loss:  0.008730163\n",
      "Iteration:  3200 , loss:  0.00860187\n",
      "Iteration:  3300 , loss:  0.008489998\n",
      "Iteration:  3400 , loss:  0.008390636\n",
      "Iteration:  3500 , loss:  0.008299367\n",
      "Iteration:  3600 , loss:  0.008246675\n",
      "Iteration:  3700 , loss:  0.008147924\n",
      "Iteration:  3800 , loss:  0.00808929\n",
      "Iteration:  3900 , loss:  0.008023939\n",
      "Iteration:  4000 , loss:  0.007968376\n",
      "Iteration:  4100 , loss:  0.00792636\n",
      "Iteration:  4200 , loss:  0.00787929\n",
      "Iteration:  4300 , loss:  0.007837926\n",
      "Iteration:  4400 , loss:  0.0078047477\n",
      "Iteration:  4500 , loss:  0.007770287\n",
      "Iteration:  4600 , loss:  0.007871413\n",
      "Iteration:  4700 , loss:  0.0077108643\n",
      "Iteration:  4800 , loss:  0.0076890187\n",
      "Iteration:  4900 , loss:  0.007918501\n",
      "Iteration:  5000 , loss:  0.007639755\n",
      "Iteration:  5100 , loss:  0.007732725\n",
      "Iteration:  5200 , loss:  0.0076011233\n",
      "Iteration:  5300 , loss:  0.00921062\n",
      "Iteration:  5400 , loss:  0.007567181\n",
      "Iteration:  5500 , loss:  0.0075523444\n",
      "Iteration:  5600 , loss:  0.0075359517\n",
      "Iteration:  5700 , loss:  0.0075379084\n",
      "Iteration:  5800 , loss:  0.007506747\n",
      "Iteration:  5900 , loss:  0.0075057377\n",
      "Iteration:  6000 , loss:  0.007481055\n",
      "Iteration:  6100 , loss:  0.007467409\n",
      "Iteration:  6200 , loss:  0.0074603865\n",
      "Iteration:  6300 , loss:  0.0074444246\n",
      "Iteration:  6400 , loss:  0.007790664\n",
      "Iteration:  6500 , loss:  0.0074235937\n",
      "Iteration:  6600 , loss:  0.0074129878\n",
      "Iteration:  6700 , loss:  0.007410953\n",
      "Iteration:  6800 , loss:  0.0073937704\n",
      "Iteration:  6900 , loss:  0.007385897\n",
      "Iteration:  7000 , loss:  0.0073752957\n",
      "Iteration:  7100 , loss:  0.007387694\n",
      "Iteration:  7200 , loss:  0.007357803\n",
      "Iteration:  7300 , loss:  0.0073502148\n",
      "Iteration:  7400 , loss:  0.007376496\n",
      "Iteration:  7500 , loss:  0.0073444727\n",
      "Iteration:  7600 , loss:  0.0073249103\n",
      "Iteration:  7700 , loss:  0.0073168827\n",
      "Iteration:  7800 , loss:  0.007587183\n",
      "Iteration:  7900 , loss:  0.007301529\n",
      "Iteration:  8000 , loss:  0.0073074074\n",
      "Iteration:  8100 , loss:  0.0072871507\n",
      "Iteration:  8200 , loss:  0.0072797565\n",
      "Iteration:  8300 , loss:  0.0072742715\n",
      "Iteration:  8400 , loss:  0.007266831\n",
      "Iteration:  8500 , loss:  0.007288305\n",
      "Iteration:  8600 , loss:  0.0072565214\n",
      "Iteration:  8700 , loss:  0.007859826\n",
      "Iteration:  8800 , loss:  0.0072430405\n",
      "Iteration:  8900 , loss:  0.007302896\n",
      "Iteration:  9000 , loss:  0.0072320257\n",
      "Iteration:  9100 , loss:  0.007976366\n",
      "Iteration:  9200 , loss:  0.0072218124\n",
      "Iteration:  9300 , loss:  0.0072393846\n",
      "Iteration:  9400 , loss:  0.007212523\n",
      "Iteration:  9500 , loss:  0.007207412\n",
      "Iteration:  9600 , loss:  0.0072023384\n",
      "Iteration:  9700 , loss:  0.00719725\n",
      "Iteration:  9800 , loss:  0.0071930806\n",
      "Iteration:  9900 , loss:  0.0071996464\n",
      "Iteration:  10000 , loss:  0.0071840305\n",
      "Iteration:  10100 , loss:  0.007267563\n",
      "Iteration:  10200 , loss:  0.007175414\n",
      "Iteration:  10300 , loss:  0.0071753287\n",
      "Iteration:  10400 , loss:  0.0071671316\n",
      "Iteration:  10500 , loss:  0.0071636625\n",
      "Iteration:  10600 , loss:  0.0071603884\n",
      "Iteration:  10700 , loss:  0.0071584545\n",
      "Iteration:  10800 , loss:  0.007151614\n",
      "Iteration:  10900 , loss:  0.0071495348\n",
      "Iteration:  11000 , loss:  0.007143883\n",
      "Iteration:  11100 , loss:  0.0071429424\n",
      "Iteration:  11200 , loss:  0.0071365945\n",
      "Iteration:  11300 , loss:  0.0071504954\n",
      "Iteration:  11400 , loss:  0.0071295616\n",
      "Iteration:  11500 , loss:  0.007125615\n",
      "Iteration:  11600 , loss:  0.007123356\n",
      "Iteration:  11700 , loss:  0.0071192863\n",
      "Iteration:  11800 , loss:  0.0071175504\n",
      "Iteration:  11900 , loss:  0.007112817\n",
      "Iteration:  12000 , loss:  0.007108955\n",
      "Iteration:  12100 , loss:  0.007128322\n",
      "Iteration:  12200 , loss:  0.0071025733\n",
      "Iteration:  12300 , loss:  0.007254039\n",
      "Iteration:  12400 , loss:  0.007129261\n",
      "Iteration:  12500 , loss:  0.0070923525\n",
      "Iteration:  12600 , loss:  0.007095424\n",
      "Iteration:  12700 , loss:  0.0070858737\n",
      "Iteration:  12800 , loss:  0.0070830053\n",
      "Iteration:  12900 , loss:  0.0070795906\n",
      "Iteration:  13000 , loss:  0.0070791175\n",
      "Iteration:  13100 , loss:  0.0070734154\n",
      "Iteration:  13200 , loss:  0.0070717884\n",
      "Iteration:  13300 , loss:  0.007067838\n",
      "Iteration:  13400 , loss:  0.007065539\n",
      "Iteration:  13500 , loss:  0.0070619\n",
      "Iteration:  13600 , loss:  0.0070593306\n",
      "Iteration:  13700 , loss:  0.0070561846\n",
      "Iteration:  13800 , loss:  0.0070530707\n",
      "Iteration:  13900 , loss:  0.007051019\n",
      "Iteration:  14000 , loss:  0.007431431\n",
      "Iteration:  14100 , loss:  0.007044872\n",
      "Iteration:  14200 , loss:  0.0070457025\n",
      "Iteration:  14300 , loss:  0.0070401845\n",
      "Iteration:  14400 , loss:  0.0070373644\n",
      "Iteration:  14500 , loss:  0.0072309235\n",
      "Iteration:  14600 , loss:  0.0070323017\n",
      "Iteration:  14700 , loss:  0.00703048\n",
      "Iteration:  14800 , loss:  0.007028347\n",
      "Iteration:  14900 , loss:  0.0070258537\n",
      "Iteration:  15000 , loss:  0.0070242384\n",
      "Iteration:  15100 , loss:  0.007040438\n",
      "Iteration:  15200 , loss:  0.007019392\n",
      "Iteration:  15300 , loss:  0.0070440555\n",
      "Iteration:  15400 , loss:  0.0070155747\n",
      "Iteration:  15500 , loss:  0.0070311883\n",
      "Iteration:  15600 , loss:  0.0070120315\n",
      "Iteration:  15700 , loss:  0.0070117693\n",
      "Iteration:  15800 , loss:  0.007031796\n",
      "Iteration:  15900 , loss:  0.0075547127\n",
      "Iteration:  16000 , loss:  0.0070055067\n",
      "Iteration:  16100 , loss:  0.0070050918\n",
      "Iteration:  16200 , loss:  0.0070025306\n",
      "Iteration:  16300 , loss:  0.0070038806\n",
      "Iteration:  16400 , loss:  0.007005012\n",
      "Iteration:  16500 , loss:  0.0070032664\n",
      "Iteration:  16600 , loss:  0.007972624\n",
      "Iteration:  16700 , loss:  0.0069959452\n",
      "Iteration:  16800 , loss:  0.007011257\n",
      "Iteration:  16900 , loss:  0.0069934726\n",
      "Iteration:  17000 , loss:  0.0070480537\n",
      "Iteration:  17100 , loss:  0.0069917333\n",
      "Iteration:  17200 , loss:  0.0072628898\n",
      "Iteration:  17300 , loss:  0.0069891084\n",
      "Iteration:  17400 , loss:  0.0069901636\n",
      "Iteration:  17500 , loss:  0.0070039756\n",
      "Iteration:  17600 , loss:  0.006985942\n",
      "Iteration:  17700 , loss:  0.0069853966\n",
      "Iteration:  17800 , loss:  0.006984603\n",
      "Iteration:  17900 , loss:  0.006983456\n",
      "Iteration:  18000 , loss:  0.007204824\n",
      "Iteration:  18100 , loss:  0.006981402\n",
      "Iteration:  18200 , loss:  0.0070016705\n",
      "Iteration:  18300 , loss:  0.0069795665\n",
      "Iteration:  18400 , loss:  0.006979216\n",
      "Iteration:  18500 , loss:  0.006978078\n",
      "Iteration:  18600 , loss:  0.00697818\n",
      "Iteration:  18700 , loss:  0.006994752\n",
      "Iteration:  18800 , loss:  0.006978638\n",
      "Iteration:  18900 , loss:  0.007271301\n",
      "Iteration:  19000 , loss:  0.0069992607\n",
      "Iteration:  19100 , loss:  0.006984887\n",
      "Iteration:  19200 , loss:  0.00697252\n",
      "Iteration:  19300 , loss:  0.0069730678\n",
      "Iteration:  19400 , loss:  0.006971146\n",
      "Iteration:  19500 , loss:  0.0069718454\n",
      "Iteration:  19600 , loss:  0.0073281205\n",
      "Iteration:  19700 , loss:  0.0069691385\n",
      "Iteration:  19800 , loss:  0.006972816\n",
      "Iteration:  19900 , loss:  0.006968509\n",
      "Generating 2th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.10326329\n",
      "Iteration:  100 , loss:  0.041914284\n",
      "Iteration:  200 , loss:  0.03838268\n",
      "Iteration:  300 , loss:  0.03676374\n",
      "Iteration:  400 , loss:  0.035177656\n",
      "Iteration:  500 , loss:  0.032907374\n",
      "Iteration:  600 , loss:  0.030154936\n",
      "Iteration:  700 , loss:  0.027393403\n",
      "Iteration:  800 , loss:  0.02520471\n",
      "Iteration:  900 , loss:  0.023450334\n",
      "Iteration:  1000 , loss:  0.022185424\n",
      "Iteration:  1100 , loss:  0.020980863\n",
      "Iteration:  1200 , loss:  0.019931745\n",
      "Iteration:  1300 , loss:  0.018957647\n",
      "Iteration:  1400 , loss:  0.018175859\n",
      "Iteration:  1500 , loss:  0.01744289\n",
      "Iteration:  1600 , loss:  0.016771544\n",
      "Iteration:  1700 , loss:  0.016132439\n",
      "Iteration:  1800 , loss:  0.015500591\n",
      "Iteration:  1900 , loss:  0.014820414\n",
      "Iteration:  2000 , loss:  0.014118943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  2100 , loss:  0.013415412\n",
      "Iteration:  2200 , loss:  0.0128239505\n",
      "Iteration:  2300 , loss:  0.012314697\n",
      "Iteration:  2400 , loss:  0.011983359\n",
      "Iteration:  2500 , loss:  0.01149106\n",
      "Iteration:  2600 , loss:  0.011141497\n",
      "Iteration:  2700 , loss:  0.010847238\n",
      "Iteration:  2800 , loss:  0.010581897\n",
      "Iteration:  2900 , loss:  0.010383137\n",
      "Iteration:  3000 , loss:  0.010156252\n",
      "Iteration:  3100 , loss:  0.009974578\n",
      "Iteration:  3200 , loss:  0.0098205935\n",
      "Iteration:  3300 , loss:  0.009669026\n",
      "Iteration:  3400 , loss:  0.009597737\n",
      "Iteration:  3500 , loss:  0.009382175\n",
      "Iteration:  3600 , loss:  0.009240609\n",
      "Iteration:  3700 , loss:  0.009110476\n",
      "Iteration:  3800 , loss:  0.008977193\n",
      "Iteration:  3900 , loss:  0.009788095\n",
      "Iteration:  4000 , loss:  0.008738706\n",
      "Iteration:  4100 , loss:  0.008630295\n",
      "Iteration:  4200 , loss:  0.008538619\n",
      "Iteration:  4300 , loss:  0.008452809\n",
      "Iteration:  4400 , loss:  0.008372318\n",
      "Iteration:  4500 , loss:  0.008302898\n",
      "Iteration:  4600 , loss:  0.008234231\n",
      "Iteration:  4700 , loss:  0.008310221\n",
      "Iteration:  4800 , loss:  0.008116597\n",
      "Iteration:  4900 , loss:  0.0080645755\n",
      "Iteration:  5000 , loss:  0.008095974\n",
      "Iteration:  5100 , loss:  0.007983035\n",
      "Iteration:  5200 , loss:  0.007947263\n",
      "Iteration:  5300 , loss:  0.00791395\n",
      "Iteration:  5400 , loss:  0.007887141\n",
      "Iteration:  5500 , loss:  0.007859896\n",
      "Iteration:  5600 , loss:  0.007867117\n",
      "Iteration:  5700 , loss:  0.0078118895\n",
      "Iteration:  5800 , loss:  0.007789043\n",
      "Iteration:  5900 , loss:  0.007913343\n",
      "Iteration:  6000 , loss:  0.007747405\n",
      "Iteration:  6100 , loss:  0.007726569\n",
      "Iteration:  6200 , loss:  0.0077089444\n",
      "Iteration:  6300 , loss:  0.007689473\n",
      "Iteration:  6400 , loss:  0.007922523\n",
      "Iteration:  6500 , loss:  0.007654389\n",
      "Iteration:  6600 , loss:  0.007636494\n",
      "Iteration:  6700 , loss:  0.007645733\n",
      "Iteration:  6800 , loss:  0.0076033617\n",
      "Iteration:  6900 , loss:  0.0075863022\n",
      "Iteration:  7000 , loss:  0.007571777\n",
      "Iteration:  7100 , loss:  0.00755542\n",
      "Iteration:  7200 , loss:  0.007784707\n",
      "Iteration:  7300 , loss:  0.007525471\n",
      "Iteration:  7400 , loss:  0.007512371\n",
      "Iteration:  7500 , loss:  0.007506496\n",
      "Iteration:  7600 , loss:  0.008192835\n",
      "Iteration:  7700 , loss:  0.0074718557\n",
      "Iteration:  7800 , loss:  0.0074628224\n",
      "Iteration:  7900 , loss:  0.007448086\n",
      "Iteration:  8000 , loss:  0.0074724653\n",
      "Iteration:  8100 , loss:  0.007677069\n",
      "Iteration:  8200 , loss:  0.007415238\n",
      "Iteration:  8300 , loss:  0.0074064448\n",
      "Iteration:  8400 , loss:  0.007538658\n",
      "Iteration:  8500 , loss:  0.0076627126\n",
      "Iteration:  8600 , loss:  0.0073759463\n",
      "Iteration:  8700 , loss:  0.0073668053\n",
      "Iteration:  8800 , loss:  0.0076847747\n",
      "Iteration:  8900 , loss:  0.007349186\n",
      "Iteration:  9000 , loss:  0.00734768\n",
      "Iteration:  9100 , loss:  0.0073326305\n",
      "Iteration:  9200 , loss:  0.0073240045\n",
      "Iteration:  9300 , loss:  0.007317091\n",
      "Iteration:  9400 , loss:  0.007308404\n",
      "Iteration:  9500 , loss:  0.007317051\n",
      "Iteration:  9600 , loss:  0.0072934395\n",
      "Iteration:  9700 , loss:  0.007324964\n",
      "Iteration:  9800 , loss:  0.0074012466\n",
      "Iteration:  9900 , loss:  0.0072711315\n",
      "Iteration:  10000 , loss:  0.007266607\n",
      "Iteration:  10100 , loss:  0.0072846124\n",
      "Iteration:  10200 , loss:  0.0072607724\n",
      "Iteration:  10300 , loss:  0.007332389\n",
      "Iteration:  10400 , loss:  0.0072363485\n",
      "Iteration:  10500 , loss:  0.0072312285\n",
      "Iteration:  10600 , loss:  0.007362605\n",
      "Iteration:  10700 , loss:  0.007217635\n",
      "Iteration:  10800 , loss:  0.0072866403\n",
      "Iteration:  10900 , loss:  0.0072053396\n",
      "Iteration:  11000 , loss:  0.007200138\n",
      "Iteration:  11100 , loss:  0.007193704\n",
      "Iteration:  11200 , loss:  0.007234525\n",
      "Iteration:  11300 , loss:  0.007184626\n",
      "Iteration:  11400 , loss:  0.007231566\n",
      "Iteration:  11500 , loss:  0.007708134\n",
      "Iteration:  11600 , loss:  0.007171571\n",
      "Iteration:  11700 , loss:  0.0072223824\n",
      "Iteration:  11800 , loss:  0.007156598\n",
      "Iteration:  11900 , loss:  0.007282286\n",
      "Iteration:  12000 , loss:  0.0071477853\n",
      "Iteration:  12100 , loss:  0.008629961\n",
      "Iteration:  12200 , loss:  0.007139694\n",
      "Iteration:  12300 , loss:  0.0071353903\n",
      "Iteration:  12400 , loss:  0.0071344525\n",
      "Iteration:  12500 , loss:  0.0071280496\n",
      "Iteration:  12600 , loss:  0.0077062403\n",
      "Iteration:  12700 , loss:  0.00712094\n",
      "Iteration:  12800 , loss:  0.00711816\n",
      "Iteration:  12900 , loss:  0.0072548506\n",
      "Iteration:  13000 , loss:  0.0071128807\n",
      "Iteration:  13100 , loss:  0.0071095405\n",
      "Iteration:  13200 , loss:  0.007175339\n",
      "Iteration:  13300 , loss:  0.0071029128\n",
      "Iteration:  13400 , loss:  0.0073753446\n",
      "Iteration:  13500 , loss:  0.0070976494\n",
      "Iteration:  13600 , loss:  0.0076077664\n",
      "Iteration:  13700 , loss:  0.007092683\n",
      "Iteration:  13800 , loss:  0.0070909206\n",
      "Iteration:  13900 , loss:  0.0070881634\n",
      "Iteration:  14000 , loss:  0.007092802\n",
      "Iteration:  14100 , loss:  0.007083196\n",
      "Iteration:  14200 , loss:  0.007081393\n",
      "Iteration:  14300 , loss:  0.0070792786\n",
      "Iteration:  14400 , loss:  0.0070785186\n",
      "Iteration:  14500 , loss:  0.0070752203\n",
      "Iteration:  14600 , loss:  0.0070729763\n",
      "Iteration:  14700 , loss:  0.007085613\n",
      "Iteration:  14800 , loss:  0.007069298\n",
      "Iteration:  14900 , loss:  0.0072074733\n",
      "Iteration:  15000 , loss:  0.0070658554\n",
      "Iteration:  15100 , loss:  0.0070638484\n",
      "Iteration:  15200 , loss:  0.007073218\n",
      "Iteration:  15300 , loss:  0.007060598\n",
      "Iteration:  15400 , loss:  0.007078246\n",
      "Iteration:  15500 , loss:  0.0070576137\n",
      "Iteration:  15600 , loss:  0.007055607\n",
      "Iteration:  15700 , loss:  0.007054957\n",
      "Iteration:  15800 , loss:  0.007056207\n",
      "Iteration:  15900 , loss:  0.0070832144\n",
      "Iteration:  16000 , loss:  0.0070492188\n",
      "Iteration:  16100 , loss:  0.0070785414\n",
      "Iteration:  16200 , loss:  0.007046178\n",
      "Iteration:  16300 , loss:  0.0082133785\n",
      "Iteration:  16400 , loss:  0.0070430255\n",
      "Iteration:  16500 , loss:  0.007331454\n",
      "Iteration:  16600 , loss:  0.007040108\n",
      "Iteration:  16700 , loss:  0.00749941\n",
      "Iteration:  16800 , loss:  0.0070372694\n",
      "Iteration:  16900 , loss:  0.0070355795\n",
      "Iteration:  17000 , loss:  0.0070365435\n",
      "Iteration:  17100 , loss:  0.0070328536\n",
      "Iteration:  17200 , loss:  0.0070729167\n",
      "Iteration:  17300 , loss:  0.0070301224\n",
      "Iteration:  17400 , loss:  0.007028654\n",
      "Iteration:  17500 , loss:  0.0070274174\n",
      "Iteration:  17600 , loss:  0.007030521\n",
      "Iteration:  17700 , loss:  0.0072159846\n",
      "Iteration:  17800 , loss:  0.0070230905\n",
      "Iteration:  17900 , loss:  0.0070228623\n",
      "Iteration:  18000 , loss:  0.0070204204\n",
      "Iteration:  18100 , loss:  0.007121532\n",
      "Iteration:  18200 , loss:  0.0070177126\n",
      "Iteration:  18300 , loss:  0.007017199\n",
      "Iteration:  18400 , loss:  0.0071127266\n",
      "Iteration:  18500 , loss:  0.007013898\n",
      "Iteration:  18600 , loss:  0.007012842\n",
      "Iteration:  18700 , loss:  0.007041832\n",
      "Iteration:  18800 , loss:  0.00704754\n",
      "Iteration:  18900 , loss:  0.007031522\n",
      "Iteration:  19000 , loss:  0.007119245\n",
      "Iteration:  19100 , loss:  0.0070067863\n",
      "Iteration:  19200 , loss:  0.0070058443\n",
      "Iteration:  19300 , loss:  0.0070221345\n",
      "Iteration:  19400 , loss:  0.00744841\n",
      "Iteration:  19500 , loss:  0.0070012747\n",
      "Iteration:  19600 , loss:  0.007000427\n",
      "Iteration:  19700 , loss:  0.006999346\n",
      "Iteration:  19800 , loss:  0.0069978824\n",
      "Iteration:  19900 , loss:  0.00709055\n",
      "Generating 3th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  1.2751338\n",
      "Iteration:  100 , loss:  0.05355206\n",
      "Iteration:  200 , loss:  0.04560311\n",
      "Iteration:  300 , loss:  0.042508293\n",
      "Iteration:  400 , loss:  0.04090735\n",
      "Iteration:  500 , loss:  0.03973092\n",
      "Iteration:  600 , loss:  0.038821273\n",
      "Iteration:  700 , loss:  0.038128074\n",
      "Iteration:  800 , loss:  0.037610427\n",
      "Iteration:  900 , loss:  0.0371897\n",
      "Iteration:  1000 , loss:  0.03679022\n",
      "Iteration:  1100 , loss:  0.036378935\n",
      "Iteration:  1200 , loss:  0.03594512\n",
      "Iteration:  1300 , loss:  0.035482787\n",
      "Iteration:  1400 , loss:  0.03498657\n",
      "Iteration:  1500 , loss:  0.03445306\n",
      "Iteration:  1600 , loss:  0.03388418\n",
      "Iteration:  1700 , loss:  0.03328778\n",
      "Iteration:  1800 , loss:  0.032667488\n",
      "Iteration:  1900 , loss:  0.032007948\n",
      "Iteration:  2000 , loss:  0.03128873\n",
      "Iteration:  2100 , loss:  0.030546775\n",
      "Iteration:  2200 , loss:  0.029842883\n",
      "Iteration:  2300 , loss:  0.029144395\n",
      "Iteration:  2400 , loss:  0.028365357\n",
      "Iteration:  2500 , loss:  0.027583327\n",
      "Iteration:  2600 , loss:  0.027076893\n",
      "Iteration:  2700 , loss:  0.026653092\n",
      "Iteration:  2800 , loss:  0.026254492\n",
      "Iteration:  2900 , loss:  0.025859222\n",
      "Iteration:  3000 , loss:  0.025456663\n",
      "Iteration:  3100 , loss:  0.025040988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  3200 , loss:  0.024607513\n",
      "Iteration:  3300 , loss:  0.0241514\n",
      "Iteration:  3400 , loss:  0.023667295\n",
      "Iteration:  3500 , loss:  0.023150112\n",
      "Iteration:  3600 , loss:  0.022600248\n",
      "Iteration:  3700 , loss:  0.022033136\n",
      "Iteration:  3800 , loss:  0.021471381\n",
      "Iteration:  3900 , loss:  0.020920962\n",
      "Iteration:  4000 , loss:  0.020365052\n",
      "Iteration:  4100 , loss:  0.019784417\n",
      "Iteration:  4200 , loss:  0.019193716\n",
      "Iteration:  4300 , loss:  0.0186058\n",
      "Iteration:  4400 , loss:  0.017995289\n",
      "Iteration:  4500 , loss:  0.017355546\n",
      "Iteration:  4600 , loss:  0.016714588\n",
      "Iteration:  4700 , loss:  0.016138874\n",
      "Iteration:  4800 , loss:  0.01561006\n",
      "Iteration:  4900 , loss:  0.015126519\n",
      "Iteration:  5000 , loss:  0.014630725\n",
      "Iteration:  5100 , loss:  0.0141686\n",
      "Iteration:  5200 , loss:  0.013730163\n",
      "Iteration:  5300 , loss:  0.013268841\n",
      "Iteration:  5400 , loss:  0.012837302\n",
      "Iteration:  5500 , loss:  0.0124058295\n",
      "Iteration:  5600 , loss:  0.01198636\n",
      "Iteration:  5700 , loss:  0.01156713\n",
      "Iteration:  5800 , loss:  0.011170119\n",
      "Iteration:  5900 , loss:  0.010798858\n",
      "Iteration:  6000 , loss:  0.011098271\n",
      "Iteration:  6100 , loss:  0.0101399105\n",
      "Iteration:  6200 , loss:  0.00985388\n",
      "Iteration:  6300 , loss:  0.009615136\n",
      "Iteration:  6400 , loss:  0.009396819\n",
      "Iteration:  6500 , loss:  0.009322987\n",
      "Iteration:  6600 , loss:  0.0090442\n",
      "Iteration:  6700 , loss:  0.0088907555\n",
      "Iteration:  6800 , loss:  0.00876556\n",
      "Iteration:  6900 , loss:  0.008634781\n",
      "Iteration:  7000 , loss:  0.0114505645\n",
      "Iteration:  7100 , loss:  0.008420863\n",
      "Iteration:  7200 , loss:  0.008323593\n",
      "Iteration:  7300 , loss:  0.008306555\n",
      "Iteration:  7400 , loss:  0.008160151\n",
      "Iteration:  7500 , loss:  0.008084272\n",
      "Iteration:  7600 , loss:  0.008021283\n",
      "Iteration:  7700 , loss:  0.007957731\n",
      "Iteration:  7800 , loss:  0.007898221\n",
      "Iteration:  7900 , loss:  0.007851136\n",
      "Iteration:  8000 , loss:  0.0078010187\n",
      "Iteration:  8100 , loss:  0.007755594\n",
      "Iteration:  8200 , loss:  0.007717523\n",
      "Iteration:  8300 , loss:  0.007678235\n",
      "Iteration:  8400 , loss:  0.0077599916\n",
      "Iteration:  8500 , loss:  0.007612605\n",
      "Iteration:  8600 , loss:  0.007581022\n",
      "Iteration:  8700 , loss:  0.0075586587\n",
      "Iteration:  8800 , loss:  0.00753113\n",
      "Iteration:  8900 , loss:  0.007523513\n",
      "Iteration:  9000 , loss:  0.0074863993\n",
      "Iteration:  9100 , loss:  0.0074644997\n",
      "Iteration:  9200 , loss:  0.007459413\n",
      "Iteration:  9300 , loss:  0.0074286107\n",
      "Iteration:  9400 , loss:  0.007464258\n",
      "Iteration:  9500 , loss:  0.0073971706\n",
      "Iteration:  9600 , loss:  0.0073813354\n",
      "Iteration:  9700 , loss:  0.00736965\n",
      "Iteration:  9800 , loss:  0.0073552066\n",
      "Iteration:  9900 , loss:  0.007341762\n",
      "Iteration:  10000 , loss:  0.0073322984\n",
      "Iteration:  10100 , loss:  0.007320349\n",
      "Iteration:  10200 , loss:  0.0076463483\n",
      "Iteration:  10300 , loss:  0.007300279\n",
      "Iteration:  10400 , loss:  0.0072898967\n",
      "Iteration:  10500 , loss:  0.007283778\n",
      "Iteration:  10600 , loss:  0.007272418\n",
      "Iteration:  10700 , loss:  0.00738968\n",
      "Iteration:  10800 , loss:  0.007256345\n",
      "Iteration:  10900 , loss:  0.0072478517\n",
      "Iteration:  11000 , loss:  0.0072422465\n",
      "Iteration:  11100 , loss:  0.0072338274\n",
      "Iteration:  11200 , loss:  0.0072285165\n",
      "Iteration:  11300 , loss:  0.007220724\n",
      "Iteration:  11400 , loss:  0.0072135124\n",
      "Iteration:  11500 , loss:  0.0072152303\n",
      "Iteration:  11600 , loss:  0.0072019263\n",
      "Iteration:  11700 , loss:  0.0071952464\n",
      "Iteration:  11800 , loss:  0.0071887956\n",
      "Iteration:  11900 , loss:  0.007184124\n",
      "Iteration:  12000 , loss:  0.007177798\n",
      "Iteration:  12100 , loss:  0.008259925\n",
      "Iteration:  12200 , loss:  0.007167509\n",
      "Iteration:  12300 , loss:  0.00716179\n",
      "Iteration:  12400 , loss:  0.0073135775\n",
      "Iteration:  12500 , loss:  0.007151871\n",
      "Iteration:  12600 , loss:  0.0071527944\n",
      "Iteration:  12700 , loss:  0.007142387\n",
      "Iteration:  12800 , loss:  0.007137266\n",
      "Iteration:  12900 , loss:  0.0071377214\n",
      "Iteration:  13000 , loss:  0.0071284976\n",
      "Iteration:  13100 , loss:  0.009376473\n",
      "Iteration:  13200 , loss:  0.007120197\n",
      "Iteration:  13300 , loss:  0.0071155517\n",
      "Iteration:  13400 , loss:  0.0071164565\n",
      "Iteration:  13500 , loss:  0.007107697\n",
      "Iteration:  13600 , loss:  0.007121773\n",
      "Iteration:  13700 , loss:  0.0071002487\n",
      "Iteration:  13800 , loss:  0.007096172\n",
      "Iteration:  13900 , loss:  0.007098981\n",
      "Iteration:  14000 , loss:  0.007089247\n",
      "Iteration:  14100 , loss:  0.008152159\n",
      "Iteration:  14200 , loss:  0.0070826155\n",
      "Iteration:  14300 , loss:  0.0070789824\n",
      "Iteration:  14400 , loss:  0.0070783203\n",
      "Iteration:  14500 , loss:  0.0070727672\n",
      "Iteration:  14600 , loss:  0.007237687\n",
      "Iteration:  14700 , loss:  0.00706693\n",
      "Iteration:  14800 , loss:  0.0070637\n",
      "Iteration:  14900 , loss:  0.00707226\n",
      "Iteration:  15000 , loss:  0.0070586978\n",
      "Iteration:  15100 , loss:  0.0070557683\n",
      "Iteration:  15200 , loss:  0.007211414\n",
      "Iteration:  15300 , loss:  0.007050805\n",
      "Iteration:  15400 , loss:  0.007048004\n",
      "Iteration:  15500 , loss:  0.007048615\n",
      "Iteration:  15600 , loss:  0.007043517\n",
      "Iteration:  15700 , loss:  0.0070409365\n",
      "Iteration:  15800 , loss:  0.007055531\n",
      "Iteration:  15900 , loss:  0.007036613\n",
      "Iteration:  16000 , loss:  0.007938086\n",
      "Iteration:  16100 , loss:  0.0070325923\n",
      "Iteration:  16200 , loss:  0.007030201\n",
      "Iteration:  16300 , loss:  0.0070356866\n",
      "Iteration:  16400 , loss:  0.007026428\n",
      "Iteration:  16500 , loss:  0.0077693574\n",
      "Iteration:  16600 , loss:  0.0070229527\n",
      "Iteration:  16700 , loss:  0.007020933\n",
      "Iteration:  16800 , loss:  0.007043118\n",
      "Iteration:  16900 , loss:  0.007017696\n",
      "Iteration:  17000 , loss:  0.007015815\n",
      "Iteration:  17100 , loss:  0.007014862\n",
      "Iteration:  17200 , loss:  0.0070128134\n",
      "Iteration:  17300 , loss:  0.007091801\n",
      "Iteration:  17400 , loss:  0.0070100026\n",
      "Iteration:  17500 , loss:  0.00700834\n",
      "Iteration:  17600 , loss:  0.0070096836\n",
      "Iteration:  17700 , loss:  0.00700579\n",
      "Iteration:  17800 , loss:  0.007106904\n",
      "Iteration:  17900 , loss:  0.0070034363\n",
      "Iteration:  18000 , loss:  0.0070019225\n",
      "Iteration:  18100 , loss:  0.007026842\n",
      "Iteration:  18200 , loss:  0.006999692\n",
      "Iteration:  18300 , loss:  0.0069983243\n",
      "Iteration:  18400 , loss:  0.0069977315\n",
      "Iteration:  18500 , loss:  0.0069961916\n",
      "Iteration:  18600 , loss:  0.00710322\n",
      "Iteration:  18700 , loss:  0.0069946186\n",
      "Iteration:  18800 , loss:  0.0069932244\n",
      "Iteration:  18900 , loss:  0.0069920365\n",
      "Iteration:  19000 , loss:  0.0069919303\n",
      "Iteration:  19100 , loss:  0.006990255\n",
      "Iteration:  19200 , loss:  0.006989451\n",
      "Iteration:  19300 , loss:  0.0069889477\n",
      "Iteration:  19400 , loss:  0.00698741\n",
      "Iteration:  19500 , loss:  0.0069968663\n",
      "Iteration:  19600 , loss:  0.0069857575\n",
      "Iteration:  19700 , loss:  0.006984717\n",
      "Iteration:  19800 , loss:  0.006985075\n",
      "Iteration:  19900 , loss:  0.006983186\n",
      "Generating 4th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.1582676\n",
      "Iteration:  100 , loss:  0.041007888\n",
      "Iteration:  200 , loss:  0.03768214\n",
      "Iteration:  300 , loss:  0.035439517\n",
      "Iteration:  400 , loss:  0.033030327\n",
      "Iteration:  500 , loss:  0.030242275\n",
      "Iteration:  600 , loss:  0.028044436\n",
      "Iteration:  700 , loss:  0.026607286\n",
      "Iteration:  800 , loss:  0.025487754\n",
      "Iteration:  900 , loss:  0.024516117\n",
      "Iteration:  1000 , loss:  0.023623273\n",
      "Iteration:  1100 , loss:  0.022768654\n",
      "Iteration:  1200 , loss:  0.021924574\n",
      "Iteration:  1300 , loss:  0.021050815\n",
      "Iteration:  1400 , loss:  0.020136772\n",
      "Iteration:  1500 , loss:  0.019277904\n",
      "Iteration:  1600 , loss:  0.018148676\n",
      "Iteration:  1700 , loss:  0.017380908\n",
      "Iteration:  1800 , loss:  0.016741209\n",
      "Iteration:  1900 , loss:  0.016122798\n",
      "Iteration:  2000 , loss:  0.01581232\n",
      "Iteration:  2100 , loss:  0.015007986\n",
      "Iteration:  2200 , loss:  0.014585106\n",
      "Iteration:  2300 , loss:  0.014155267\n",
      "Iteration:  2400 , loss:  0.013767326\n",
      "Iteration:  2500 , loss:  0.013379378\n",
      "Iteration:  2600 , loss:  0.012995745\n",
      "Iteration:  2700 , loss:  0.012645171\n",
      "Iteration:  2800 , loss:  0.012290674\n",
      "Iteration:  2900 , loss:  0.011901356\n",
      "Iteration:  3000 , loss:  0.011479149\n",
      "Iteration:  3100 , loss:  0.0109823365\n",
      "Iteration:  3200 , loss:  0.010652255\n",
      "Iteration:  3300 , loss:  0.009996651\n",
      "Iteration:  3400 , loss:  0.009571701\n",
      "Iteration:  3500 , loss:  0.009253549\n",
      "Iteration:  3600 , loss:  0.008983937\n",
      "Iteration:  3700 , loss:  0.008792374\n",
      "Iteration:  3800 , loss:  0.008624217\n",
      "Iteration:  3900 , loss:  0.00847987\n",
      "Iteration:  4000 , loss:  0.008360311\n",
      "Iteration:  4100 , loss:  0.008254422\n",
      "Iteration:  4200 , loss:  0.008161096\n",
      "Iteration:  4300 , loss:  0.008085986\n",
      "Iteration:  4400 , loss:  0.0080159865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  4500 , loss:  0.008549407\n",
      "Iteration:  4600 , loss:  0.007902497\n",
      "Iteration:  4700 , loss:  0.007853885\n",
      "Iteration:  4800 , loss:  0.0078164\n",
      "Iteration:  4900 , loss:  0.007778691\n",
      "Iteration:  5000 , loss:  0.0077455416\n",
      "Iteration:  5100 , loss:  0.007720046\n",
      "Iteration:  5200 , loss:  0.007693908\n",
      "Iteration:  5300 , loss:  0.007688622\n",
      "Iteration:  5400 , loss:  0.0076522087\n",
      "Iteration:  5500 , loss:  0.007633062\n",
      "Iteration:  5600 , loss:  0.0077281725\n",
      "Iteration:  5700 , loss:  0.0076017566\n",
      "Iteration:  5800 , loss:  0.007586777\n",
      "Iteration:  5900 , loss:  0.0076006893\n",
      "Iteration:  6000 , loss:  0.0075615556\n",
      "Iteration:  6100 , loss:  0.00754915\n",
      "Iteration:  6200 , loss:  0.0075601847\n",
      "Iteration:  6300 , loss:  0.0075277262\n",
      "Iteration:  6400 , loss:  0.0075168805\n",
      "Iteration:  6500 , loss:  0.00751488\n",
      "Iteration:  6600 , loss:  0.0074978755\n",
      "Iteration:  6700 , loss:  0.0075822724\n",
      "Iteration:  6800 , loss:  0.0077629937\n",
      "Iteration:  6900 , loss:  0.007470171\n",
      "Iteration:  7000 , loss:  0.0074630775\n",
      "Iteration:  7100 , loss:  0.007453264\n",
      "Iteration:  7200 , loss:  0.0074640303\n",
      "Iteration:  7300 , loss:  0.0074373437\n",
      "Iteration:  7400 , loss:  0.0074290135\n",
      "Iteration:  7500 , loss:  0.007422126\n",
      "Iteration:  7600 , loss:  0.007414025\n",
      "Iteration:  7700 , loss:  0.008475151\n",
      "Iteration:  7800 , loss:  0.007398964\n",
      "Iteration:  7900 , loss:  0.007474305\n",
      "Iteration:  8000 , loss:  0.0073922314\n",
      "Iteration:  8100 , loss:  0.0073763034\n",
      "Iteration:  8200 , loss:  0.0073700333\n",
      "Iteration:  8300 , loss:  0.0073619606\n",
      "Iteration:  8400 , loss:  0.0073561384\n",
      "Iteration:  8500 , loss:  0.007348049\n",
      "Iteration:  8600 , loss:  0.0073415516\n",
      "Iteration:  8700 , loss:  0.0073341187\n",
      "Iteration:  8800 , loss:  0.007328261\n",
      "Iteration:  8900 , loss:  0.007320864\n",
      "Iteration:  9000 , loss:  0.0073203375\n",
      "Iteration:  9100 , loss:  0.008160064\n",
      "Iteration:  9200 , loss:  0.0073013953\n",
      "Iteration:  9300 , loss:  0.007296047\n",
      "Iteration:  9400 , loss:  0.007357521\n",
      "Iteration:  9500 , loss:  0.0072903587\n",
      "Iteration:  9600 , loss:  0.007276946\n",
      "Iteration:  9700 , loss:  0.0072715343\n",
      "Iteration:  9800 , loss:  0.0073402864\n",
      "Iteration:  9900 , loss:  0.0072592366\n",
      "Iteration:  10000 , loss:  0.007256438\n",
      "Iteration:  10100 , loss:  0.0072481795\n",
      "Iteration:  10200 , loss:  0.007292269\n",
      "Iteration:  10300 , loss:  0.007237684\n",
      "Iteration:  10400 , loss:  0.0072752205\n",
      "Iteration:  10500 , loss:  0.0072396323\n",
      "Iteration:  10600 , loss:  0.007249188\n",
      "Iteration:  10700 , loss:  0.0072172363\n",
      "Iteration:  10800 , loss:  0.0072131394\n",
      "Iteration:  10900 , loss:  0.007208351\n",
      "Iteration:  11000 , loss:  0.007415119\n",
      "Iteration:  11100 , loss:  0.0072299475\n",
      "Iteration:  11200 , loss:  0.007217656\n",
      "Iteration:  11300 , loss:  0.007203754\n",
      "Iteration:  11400 , loss:  0.007192207\n",
      "Iteration:  11500 , loss:  0.0072088297\n",
      "Iteration:  11600 , loss:  0.007178893\n",
      "Iteration:  11700 , loss:  0.0071718995\n",
      "Iteration:  11800 , loss:  0.0071687684\n",
      "Iteration:  11900 , loss:  0.007168862\n",
      "Iteration:  12000 , loss:  0.00716022\n",
      "Iteration:  12100 , loss:  0.007155908\n",
      "Iteration:  12200 , loss:  0.0071530105\n",
      "Iteration:  12300 , loss:  0.0071489075\n",
      "Iteration:  12400 , loss:  0.0072345184\n",
      "Iteration:  12500 , loss:  0.0071412516\n",
      "Iteration:  12600 , loss:  0.007167493\n",
      "Iteration:  12700 , loss:  0.007134121\n",
      "Iteration:  12800 , loss:  0.007154068\n",
      "Iteration:  12900 , loss:  0.007888767\n",
      "Iteration:  13000 , loss:  0.007123877\n",
      "Iteration:  13100 , loss:  0.007447196\n",
      "Iteration:  13200 , loss:  0.0071173264\n",
      "Iteration:  13300 , loss:  0.0071370797\n",
      "Iteration:  13400 , loss:  0.0071106288\n",
      "Iteration:  13500 , loss:  0.0071081663\n",
      "Iteration:  13600 , loss:  0.0071044303\n",
      "Iteration:  13700 , loss:  0.0071133687\n",
      "Iteration:  13800 , loss:  0.0070984005\n",
      "Iteration:  13900 , loss:  0.007134378\n",
      "Iteration:  14000 , loss:  0.0077302367\n",
      "Iteration:  14100 , loss:  0.0070894742\n",
      "Iteration:  14200 , loss:  0.0071223197\n",
      "Iteration:  14300 , loss:  0.007083679\n",
      "Iteration:  14400 , loss:  0.007101817\n",
      "Iteration:  14500 , loss:  0.007078107\n",
      "Iteration:  14600 , loss:  0.00708187\n",
      "Iteration:  14700 , loss:  0.0070727426\n",
      "Iteration:  14800 , loss:  0.0071216454\n",
      "Iteration:  14900 , loss:  0.007067515\n",
      "Iteration:  15000 , loss:  0.0070695016\n",
      "Iteration:  15100 , loss:  0.007250894\n",
      "Iteration:  15200 , loss:  0.0073132594\n",
      "Iteration:  15300 , loss:  0.007115998\n",
      "Iteration:  15400 , loss:  0.0070553473\n",
      "Iteration:  15500 , loss:  0.0071034357\n",
      "Iteration:  15600 , loss:  0.007049898\n",
      "Iteration:  15700 , loss:  0.0071792607\n",
      "Iteration:  15800 , loss:  0.0070452653\n",
      "Iteration:  15900 , loss:  0.007055489\n",
      "Iteration:  16000 , loss:  0.0070410725\n",
      "Iteration:  16100 , loss:  0.0070386445\n",
      "Iteration:  16200 , loss:  0.007121681\n",
      "Iteration:  16300 , loss:  0.0070354096\n",
      "Iteration:  16400 , loss:  0.0070327986\n",
      "Iteration:  16500 , loss:  0.0072079566\n",
      "Iteration:  16600 , loss:  0.0070286193\n",
      "Iteration:  16700 , loss:  0.007050019\n",
      "Iteration:  16800 , loss:  0.0070248675\n",
      "Iteration:  16900 , loss:  0.0070252274\n",
      "Iteration:  17000 , loss:  0.007021396\n",
      "Iteration:  17100 , loss:  0.0070267133\n",
      "Iteration:  17200 , loss:  0.0070181475\n",
      "Iteration:  17300 , loss:  0.0070189275\n",
      "Iteration:  17400 , loss:  0.00701653\n",
      "Iteration:  17500 , loss:  0.007707254\n",
      "Iteration:  17600 , loss:  0.007011949\n",
      "Iteration:  17700 , loss:  0.007107522\n",
      "Iteration:  17800 , loss:  0.0070422804\n",
      "Iteration:  17900 , loss:  0.0070074205\n",
      "Iteration:  18000 , loss:  0.0070063905\n",
      "Iteration:  18100 , loss:  0.007037069\n",
      "Iteration:  18200 , loss:  0.007005463\n",
      "Iteration:  18300 , loss:  0.0070066773\n",
      "Iteration:  18400 , loss:  0.0070007755\n",
      "Iteration:  18500 , loss:  0.007017763\n",
      "Iteration:  18600 , loss:  0.0070098927\n",
      "Iteration:  18700 , loss:  0.0070010093\n",
      "Iteration:  18800 , loss:  0.0069961436\n",
      "Iteration:  18900 , loss:  0.00699473\n",
      "Iteration:  19000 , loss:  0.006994985\n",
      "Iteration:  19100 , loss:  0.0069925636\n",
      "Iteration:  19200 , loss:  0.0070066573\n",
      "Iteration:  19300 , loss:  0.006990567\n",
      "Iteration:  19400 , loss:  0.0069893445\n",
      "Iteration:  19500 , loss:  0.006988602\n",
      "Iteration:  19600 , loss:  0.0069875983\n",
      "Iteration:  19700 , loss:  0.0069918516\n",
      "Iteration:  19800 , loss:  0.0069854422\n",
      "Iteration:  19900 , loss:  0.006984528\n",
      "Generating 5th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.08159003\n",
      "Iteration:  100 , loss:  0.040001914\n",
      "Iteration:  200 , loss:  0.037100162\n",
      "Iteration:  300 , loss:  0.035259362\n",
      "Iteration:  400 , loss:  0.033494562\n",
      "Iteration:  500 , loss:  0.03154826\n",
      "Iteration:  600 , loss:  0.029109573\n",
      "Iteration:  700 , loss:  0.027028058\n",
      "Iteration:  800 , loss:  0.025369477\n",
      "Iteration:  900 , loss:  0.023764264\n",
      "Iteration:  1000 , loss:  0.022184182\n",
      "Iteration:  1100 , loss:  0.020546313\n",
      "Iteration:  1200 , loss:  0.019244894\n",
      "Iteration:  1300 , loss:  0.01827842\n",
      "Iteration:  1400 , loss:  0.017449677\n",
      "Iteration:  1500 , loss:  0.016695257\n",
      "Iteration:  1600 , loss:  0.016014485\n",
      "Iteration:  1700 , loss:  0.015792046\n",
      "Iteration:  1800 , loss:  0.01471087\n",
      "Iteration:  1900 , loss:  0.014118439\n",
      "Iteration:  2000 , loss:  0.013544602\n",
      "Iteration:  2100 , loss:  0.0130063575\n",
      "Iteration:  2200 , loss:  0.01416835\n",
      "Iteration:  2300 , loss:  0.011980917\n",
      "Iteration:  2400 , loss:  0.011746297\n",
      "Iteration:  2500 , loss:  0.0110795805\n",
      "Iteration:  2600 , loss:  0.011080824\n",
      "Iteration:  2700 , loss:  0.01036583\n",
      "Iteration:  2800 , loss:  0.010066603\n",
      "Iteration:  2900 , loss:  0.009823306\n",
      "Iteration:  3000 , loss:  0.009588838\n",
      "Iteration:  3100 , loss:  0.012068899\n",
      "Iteration:  3200 , loss:  0.009228551\n",
      "Iteration:  3300 , loss:  0.0090750465\n",
      "Iteration:  3400 , loss:  0.008934829\n",
      "Iteration:  3500 , loss:  0.008827674\n",
      "Iteration:  3600 , loss:  0.008717789\n",
      "Iteration:  3700 , loss:  0.008617146\n",
      "Iteration:  3800 , loss:  0.008535938\n",
      "Iteration:  3900 , loss:  0.008453167\n",
      "Iteration:  4000 , loss:  0.00838848\n",
      "Iteration:  4100 , loss:  0.008317213\n",
      "Iteration:  4200 , loss:  0.008932069\n",
      "Iteration:  4300 , loss:  0.008202162\n",
      "Iteration:  4400 , loss:  0.00814706\n",
      "Iteration:  4500 , loss:  0.008103649\n",
      "Iteration:  4600 , loss:  0.008055432\n",
      "Iteration:  4700 , loss:  0.008010118\n",
      "Iteration:  4800 , loss:  0.007975068\n",
      "Iteration:  4900 , loss:  0.007935392\n",
      "Iteration:  5000 , loss:  0.007906141\n",
      "Iteration:  5100 , loss:  0.007870322\n",
      "Iteration:  5200 , loss:  0.007837925\n",
      "Iteration:  5300 , loss:  0.00781682\n",
      "Iteration:  5400 , loss:  0.0077855275\n",
      "Iteration:  5500 , loss:  0.0077670775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  5600 , loss:  0.00774013\n",
      "Iteration:  5700 , loss:  0.0077175526\n",
      "Iteration:  5800 , loss:  0.007740161\n",
      "Iteration:  5900 , loss:  0.0076817246\n",
      "Iteration:  6000 , loss:  0.007663492\n",
      "Iteration:  6100 , loss:  0.007651508\n",
      "Iteration:  6200 , loss:  0.007635602\n",
      "Iteration:  6300 , loss:  0.007623052\n",
      "Iteration:  6400 , loss:  0.0076109017\n",
      "Iteration:  6500 , loss:  0.0075980355\n",
      "Iteration:  6600 , loss:  0.008089195\n",
      "Iteration:  6700 , loss:  0.0075775464\n",
      "Iteration:  6800 , loss:  0.0075664762\n",
      "Iteration:  6900 , loss:  0.0075617214\n",
      "Iteration:  7000 , loss:  0.007548951\n",
      "Iteration:  7100 , loss:  0.007539044\n",
      "Iteration:  7200 , loss:  0.0075319754\n",
      "Iteration:  7300 , loss:  0.0075225015\n",
      "Iteration:  7400 , loss:  0.0075160256\n",
      "Iteration:  7500 , loss:  0.0075066388\n",
      "Iteration:  7600 , loss:  0.0074974094\n",
      "Iteration:  7700 , loss:  0.007491614\n",
      "Iteration:  7800 , loss:  0.007481442\n",
      "Iteration:  7900 , loss:  0.007490571\n",
      "Iteration:  8000 , loss:  0.0074654557\n",
      "Iteration:  8100 , loss:  0.0074562468\n",
      "Iteration:  8200 , loss:  0.0074517457\n",
      "Iteration:  8300 , loss:  0.0074404995\n",
      "Iteration:  8400 , loss:  0.0075597744\n",
      "Iteration:  8500 , loss:  0.007425716\n",
      "Iteration:  8600 , loss:  0.0074174833\n",
      "Iteration:  8700 , loss:  0.007414983\n",
      "Iteration:  8800 , loss:  0.0074037826\n",
      "Iteration:  8900 , loss:  0.0075067612\n",
      "Iteration:  9000 , loss:  0.00739113\n",
      "Iteration:  9100 , loss:  0.007385208\n",
      "Iteration:  9200 , loss:  0.007379447\n",
      "Iteration:  9300 , loss:  0.007372724\n",
      "Iteration:  9400 , loss:  0.0075613437\n",
      "Iteration:  9500 , loss:  0.0073619047\n",
      "Iteration:  9600 , loss:  0.0073556025\n",
      "Iteration:  9700 , loss:  0.007353181\n",
      "Iteration:  9800 , loss:  0.0073454133\n",
      "Iteration:  9900 , loss:  0.008410413\n",
      "Iteration:  10000 , loss:  0.007335583\n",
      "Iteration:  10100 , loss:  0.0073297033\n",
      "Iteration:  10200 , loss:  0.0073425337\n",
      "Iteration:  10300 , loss:  0.0073205866\n",
      "Iteration:  10400 , loss:  0.0073149567\n",
      "Iteration:  10500 , loss:  0.0074994233\n",
      "Iteration:  10600 , loss:  0.0073058396\n",
      "Iteration:  10700 , loss:  0.0073002963\n",
      "Iteration:  10800 , loss:  0.0073186695\n",
      "Iteration:  10900 , loss:  0.007291087\n",
      "Iteration:  11000 , loss:  0.0072876355\n",
      "Iteration:  11100 , loss:  0.00728202\n",
      "Iteration:  11200 , loss:  0.0072762063\n",
      "Iteration:  11300 , loss:  0.007276372\n",
      "Iteration:  11400 , loss:  0.0072675087\n",
      "Iteration:  11500 , loss:  0.007262156\n",
      "Iteration:  11600 , loss:  0.007278589\n",
      "Iteration:  11700 , loss:  0.007253116\n",
      "Iteration:  11800 , loss:  0.007548433\n",
      "Iteration:  11900 , loss:  0.007244386\n",
      "Iteration:  12000 , loss:  0.00723914\n",
      "Iteration:  12100 , loss:  0.0072636716\n",
      "Iteration:  12200 , loss:  0.007230733\n",
      "Iteration:  12300 , loss:  0.0072257067\n",
      "Iteration:  12400 , loss:  0.007230655\n",
      "Iteration:  12500 , loss:  0.0072180275\n",
      "Iteration:  12600 , loss:  0.007213232\n",
      "Iteration:  12700 , loss:  0.0072169118\n",
      "Iteration:  12800 , loss:  0.0072055585\n",
      "Iteration:  12900 , loss:  0.0074840216\n",
      "Iteration:  13000 , loss:  0.007198343\n",
      "Iteration:  13100 , loss:  0.007193752\n",
      "Iteration:  13200 , loss:  0.0072153066\n",
      "Iteration:  13300 , loss:  0.007186942\n",
      "Iteration:  13400 , loss:  0.007182784\n",
      "Iteration:  13500 , loss:  0.0072097173\n",
      "Iteration:  13600 , loss:  0.007176952\n",
      "Iteration:  13700 , loss:  0.007173064\n",
      "Iteration:  13800 , loss:  0.007637843\n",
      "Iteration:  13900 , loss:  0.0071673575\n",
      "Iteration:  14000 , loss:  0.007163624\n",
      "Iteration:  14100 , loss:  0.0073082377\n",
      "Iteration:  14200 , loss:  0.0071581304\n",
      "Iteration:  14300 , loss:  0.0071545797\n",
      "Iteration:  14400 , loss:  0.007168861\n",
      "Iteration:  14500 , loss:  0.0071492977\n",
      "Iteration:  14600 , loss:  0.0071501504\n",
      "Iteration:  14700 , loss:  0.007144592\n",
      "Iteration:  14800 , loss:  0.007140971\n",
      "Iteration:  14900 , loss:  0.007293341\n",
      "Iteration:  15000 , loss:  0.0071360897\n",
      "Iteration:  15100 , loss:  0.007132932\n",
      "Iteration:  15200 , loss:  0.0071324594\n",
      "Iteration:  15300 , loss:  0.00712829\n",
      "Iteration:  15400 , loss:  0.0072379857\n",
      "Iteration:  15500 , loss:  0.0071238945\n",
      "Iteration:  15600 , loss:  0.0071209036\n",
      "Iteration:  15700 , loss:  0.0071241143\n",
      "Iteration:  15800 , loss:  0.0071167042\n",
      "Iteration:  15900 , loss:  0.007430621\n",
      "Iteration:  16000 , loss:  0.0071128984\n",
      "Iteration:  16100 , loss:  0.007110078\n",
      "Iteration:  16200 , loss:  0.008337796\n",
      "Iteration:  16300 , loss:  0.0071062306\n",
      "Iteration:  16400 , loss:  0.0071034883\n",
      "Iteration:  16500 , loss:  0.0071232487\n",
      "Iteration:  16600 , loss:  0.007099932\n",
      "Iteration:  16700 , loss:  0.0070972894\n",
      "Iteration:  16800 , loss:  0.007097162\n",
      "Iteration:  16900 , loss:  0.0070942235\n",
      "Iteration:  17000 , loss:  0.0070917923\n",
      "Iteration:  17100 , loss:  0.0070925895\n",
      "Iteration:  17200 , loss:  0.0070886053\n",
      "Iteration:  17300 , loss:  0.0070871264\n",
      "Iteration:  17400 , loss:  0.0070854183\n",
      "Iteration:  17500 , loss:  0.00708302\n",
      "Iteration:  17600 , loss:  0.007233892\n",
      "Iteration:  17700 , loss:  0.007080062\n",
      "Iteration:  17800 , loss:  0.0070778015\n",
      "Iteration:  17900 , loss:  0.0070776953\n",
      "Iteration:  18000 , loss:  0.0070749726\n",
      "Iteration:  18100 , loss:  0.0070729787\n",
      "Iteration:  18200 , loss:  0.0070721684\n",
      "Iteration:  18300 , loss:  0.0070698713\n",
      "Iteration:  18400 , loss:  0.007076666\n",
      "Iteration:  18500 , loss:  0.0070671346\n",
      "Iteration:  18600 , loss:  0.007065081\n",
      "Iteration:  18700 , loss:  0.0070651337\n",
      "Iteration:  18800 , loss:  0.007062565\n",
      "Iteration:  18900 , loss:  0.007846214\n",
      "Iteration:  19000 , loss:  0.007060309\n",
      "Iteration:  19100 , loss:  0.0070582996\n",
      "Iteration:  19200 , loss:  0.0077697877\n",
      "Iteration:  19300 , loss:  0.007055848\n",
      "Iteration:  19400 , loss:  0.00705394\n",
      "Iteration:  19500 , loss:  0.007056078\n",
      "Iteration:  19600 , loss:  0.0070515014\n",
      "Iteration:  19700 , loss:  0.007101628\n",
      "Iteration:  19800 , loss:  0.0070491293\n",
      "Iteration:  19900 , loss:  0.007047572\n",
      "Generating 6th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.1370722\n",
      "Iteration:  100 , loss:  0.040501896\n",
      "Iteration:  200 , loss:  0.038304947\n",
      "Iteration:  300 , loss:  0.03673014\n",
      "Iteration:  400 , loss:  0.0352018\n",
      "Iteration:  500 , loss:  0.033695556\n",
      "Iteration:  600 , loss:  0.031576052\n",
      "Iteration:  700 , loss:  0.028873028\n",
      "Iteration:  800 , loss:  0.027125528\n",
      "Iteration:  900 , loss:  0.025845079\n",
      "Iteration:  1000 , loss:  0.024764802\n",
      "Iteration:  1100 , loss:  0.023653837\n",
      "Iteration:  1200 , loss:  0.022218829\n",
      "Iteration:  1300 , loss:  0.02100331\n",
      "Iteration:  1400 , loss:  0.019758126\n",
      "Iteration:  1500 , loss:  0.018867793\n",
      "Iteration:  1600 , loss:  0.018127576\n",
      "Iteration:  1700 , loss:  0.01747126\n",
      "Iteration:  1800 , loss:  0.016817547\n",
      "Iteration:  1900 , loss:  0.016207999\n",
      "Iteration:  2000 , loss:  0.015640883\n",
      "Iteration:  2100 , loss:  0.015138382\n",
      "Iteration:  2200 , loss:  0.014666403\n",
      "Iteration:  2300 , loss:  0.014243549\n",
      "Iteration:  2400 , loss:  0.013827303\n",
      "Iteration:  2500 , loss:  0.013448754\n",
      "Iteration:  2600 , loss:  0.013089023\n",
      "Iteration:  2700 , loss:  0.013581015\n",
      "Iteration:  2800 , loss:  0.012405675\n",
      "Iteration:  2900 , loss:  0.012073127\n",
      "Iteration:  3000 , loss:  0.011769891\n",
      "Iteration:  3100 , loss:  0.011467444\n",
      "Iteration:  3200 , loss:  0.011208915\n",
      "Iteration:  3300 , loss:  0.010885037\n",
      "Iteration:  3400 , loss:  0.011599818\n",
      "Iteration:  3500 , loss:  0.010315752\n",
      "Iteration:  3600 , loss:  0.010049142\n",
      "Iteration:  3700 , loss:  0.009823408\n",
      "Iteration:  3800 , loss:  0.009609327\n",
      "Iteration:  3900 , loss:  0.009486114\n",
      "Iteration:  4000 , loss:  0.009275379\n",
      "Iteration:  4100 , loss:  0.009143529\n",
      "Iteration:  4200 , loss:  0.009041677\n",
      "Iteration:  4300 , loss:  0.008941427\n",
      "Iteration:  4400 , loss:  0.009297604\n",
      "Iteration:  4500 , loss:  0.00877874\n",
      "Iteration:  4600 , loss:  0.008703287\n",
      "Iteration:  4700 , loss:  0.008651285\n",
      "Iteration:  4800 , loss:  0.008572029\n",
      "Iteration:  4900 , loss:  0.008508398\n",
      "Iteration:  5000 , loss:  0.008453574\n",
      "Iteration:  5100 , loss:  0.008397749\n",
      "Iteration:  5200 , loss:  0.008359786\n",
      "Iteration:  5300 , loss:  0.008299817\n",
      "Iteration:  5400 , loss:  0.008253437\n",
      "Iteration:  5500 , loss:  0.008214535\n",
      "Iteration:  5600 , loss:  0.008174661\n",
      "Iteration:  5700 , loss:  0.0082940385\n",
      "Iteration:  5800 , loss:  0.008105691\n",
      "Iteration:  5900 , loss:  0.008072149\n",
      "Iteration:  6000 , loss:  0.008044041\n",
      "Iteration:  6100 , loss:  0.008013625\n",
      "Iteration:  6200 , loss:  0.0111883525\n",
      "Iteration:  6300 , loss:  0.007961429\n",
      "Iteration:  6400 , loss:  0.007935754\n",
      "Iteration:  6500 , loss:  0.00791465\n",
      "Iteration:  6600 , loss:  0.007888641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  6700 , loss:  0.007864708\n",
      "Iteration:  6800 , loss:  0.0078842295\n",
      "Iteration:  6900 , loss:  0.007821349\n",
      "Iteration:  7000 , loss:  0.0077992175\n",
      "Iteration:  7100 , loss:  0.007784484\n",
      "Iteration:  7200 , loss:  0.0077605625\n",
      "Iteration:  7300 , loss:  0.007740036\n",
      "Iteration:  7400 , loss:  0.0077233044\n",
      "Iteration:  7500 , loss:  0.0077031706\n",
      "Iteration:  7600 , loss:  0.00774139\n",
      "Iteration:  7700 , loss:  0.007668244\n",
      "Iteration:  7800 , loss:  0.0076504815\n",
      "Iteration:  7900 , loss:  0.0076516997\n",
      "Iteration:  8000 , loss:  0.0076184813\n",
      "Iteration:  8100 , loss:  0.0076020123\n",
      "Iteration:  8200 , loss:  0.0075899577\n",
      "Iteration:  8300 , loss:  0.007572811\n",
      "Iteration:  8400 , loss:  0.0075575737\n",
      "Iteration:  8500 , loss:  0.0075461236\n",
      "Iteration:  8600 , loss:  0.007530363\n",
      "Iteration:  8700 , loss:  0.008959784\n",
      "Iteration:  8800 , loss:  0.007504543\n",
      "Iteration:  8900 , loss:  0.0074912137\n",
      "Iteration:  9000 , loss:  0.0074946787\n",
      "Iteration:  9100 , loss:  0.007466766\n",
      "Iteration:  9200 , loss:  0.007464499\n",
      "Iteration:  9300 , loss:  0.0074432436\n",
      "Iteration:  9400 , loss:  0.0074307797\n",
      "Iteration:  9500 , loss:  0.007428829\n",
      "Iteration:  9600 , loss:  0.0074083833\n",
      "Iteration:  9700 , loss:  0.0074887755\n",
      "Iteration:  9800 , loss:  0.007386803\n",
      "Iteration:  9900 , loss:  0.007375586\n",
      "Iteration:  10000 , loss:  0.0073824357\n",
      "Iteration:  10100 , loss:  0.0073551433\n",
      "Iteration:  10200 , loss:  0.0073518525\n",
      "Iteration:  10300 , loss:  0.007335679\n",
      "Iteration:  10400 , loss:  0.00852689\n",
      "Iteration:  10500 , loss:  0.0073174853\n",
      "Iteration:  10600 , loss:  0.007308101\n",
      "Iteration:  10700 , loss:  0.007303201\n",
      "Iteration:  10800 , loss:  0.0072919815\n",
      "Iteration:  10900 , loss:  0.0072837397\n",
      "Iteration:  11000 , loss:  0.007277401\n",
      "Iteration:  11100 , loss:  0.0072684377\n",
      "Iteration:  11200 , loss:  0.0073109283\n",
      "Iteration:  11300 , loss:  0.0072542173\n",
      "Iteration:  11400 , loss:  0.007249171\n",
      "Iteration:  11500 , loss:  0.0072408295\n",
      "Iteration:  11600 , loss:  0.0072334725\n",
      "Iteration:  11700 , loss:  0.007238251\n",
      "Iteration:  11800 , loss:  0.0072207637\n",
      "Iteration:  11900 , loss:  0.0072226394\n",
      "Iteration:  12000 , loss:  0.0072088586\n",
      "Iteration:  12100 , loss:  0.0072024902\n",
      "Iteration:  12200 , loss:  0.0072030667\n",
      "Iteration:  12300 , loss:  0.007191048\n",
      "Iteration:  12400 , loss:  0.008023325\n",
      "Iteration:  12500 , loss:  0.007180012\n",
      "Iteration:  12600 , loss:  0.0071741566\n",
      "Iteration:  12700 , loss:  0.0071765324\n",
      "Iteration:  12800 , loss:  0.00716428\n",
      "Iteration:  12900 , loss:  0.007158753\n",
      "Iteration:  13000 , loss:  0.0071728677\n",
      "Iteration:  13100 , loss:  0.0071489452\n",
      "Iteration:  13200 , loss:  0.0072037783\n",
      "Iteration:  13300 , loss:  0.0071395496\n",
      "Iteration:  13400 , loss:  0.0071343593\n",
      "Iteration:  13500 , loss:  0.007178461\n",
      "Iteration:  13600 , loss:  0.007125349\n",
      "Iteration:  13700 , loss:  0.007120429\n",
      "Iteration:  13800 , loss:  0.0071167317\n",
      "Iteration:  13900 , loss:  0.0071120583\n",
      "Iteration:  14000 , loss:  0.0071074446\n",
      "Iteration:  14100 , loss:  0.0071231145\n",
      "Iteration:  14200 , loss:  0.007099275\n",
      "Iteration:  14300 , loss:  0.0070947995\n",
      "Iteration:  14400 , loss:  0.0071422127\n",
      "Iteration:  14500 , loss:  0.0070879813\n",
      "Iteration:  14600 , loss:  0.007084188\n",
      "Iteration:  14700 , loss:  0.007080319\n",
      "Iteration:  14800 , loss:  0.007082777\n",
      "Iteration:  14900 , loss:  0.007073324\n",
      "Iteration:  15000 , loss:  0.008314913\n",
      "Iteration:  15100 , loss:  0.007066537\n",
      "Iteration:  15200 , loss:  0.007062854\n",
      "Iteration:  15300 , loss:  0.007061668\n",
      "Iteration:  15400 , loss:  0.007056349\n",
      "Iteration:  15500 , loss:  0.007339881\n",
      "Iteration:  15600 , loss:  0.007050136\n",
      "Iteration:  15700 , loss:  0.007046809\n",
      "Iteration:  15800 , loss:  0.0070448243\n",
      "Iteration:  15900 , loss:  0.0070412024\n",
      "Iteration:  16000 , loss:  0.0070438697\n",
      "Iteration:  16100 , loss:  0.007104581\n",
      "Iteration:  16200 , loss:  0.0070456294\n",
      "Iteration:  16300 , loss:  0.007030389\n",
      "Iteration:  16400 , loss:  0.007073406\n",
      "Iteration:  16500 , loss:  0.0070253965\n",
      "Iteration:  16600 , loss:  0.0070425393\n",
      "Iteration:  16700 , loss:  0.0070208297\n",
      "Iteration:  16800 , loss:  0.00728775\n",
      "Iteration:  16900 , loss:  0.007016656\n",
      "Iteration:  17000 , loss:  0.007015042\n",
      "Iteration:  17100 , loss:  0.0070176087\n",
      "Iteration:  17200 , loss:  0.007010557\n",
      "Iteration:  17300 , loss:  0.007014618\n",
      "Iteration:  17400 , loss:  0.007007579\n",
      "Iteration:  17500 , loss:  0.0070337816\n",
      "Iteration:  17600 , loss:  0.007003696\n",
      "Iteration:  17700 , loss:  0.007002142\n",
      "Iteration:  17800 , loss:  0.007142707\n",
      "Iteration:  17900 , loss:  0.006999046\n",
      "Iteration:  18000 , loss:  0.007119792\n",
      "Iteration:  18100 , loss:  0.006996261\n",
      "Iteration:  18200 , loss:  0.0076161353\n",
      "Iteration:  18300 , loss:  0.006993792\n",
      "Iteration:  18400 , loss:  0.0070266835\n",
      "Iteration:  18500 , loss:  0.0069911475\n",
      "Iteration:  18600 , loss:  0.0069908476\n",
      "Iteration:  18700 , loss:  0.0071746483\n",
      "Iteration:  18800 , loss:  0.006987739\n",
      "Iteration:  18900 , loss:  0.0070201033\n",
      "Iteration:  19000 , loss:  0.006985649\n",
      "Iteration:  19100 , loss:  0.0069897366\n",
      "Iteration:  19200 , loss:  0.006983674\n",
      "Iteration:  19300 , loss:  0.0069872458\n",
      "Iteration:  19400 , loss:  0.0069818567\n",
      "Iteration:  19500 , loss:  0.007038844\n",
      "Iteration:  19600 , loss:  0.0069997828\n",
      "Iteration:  19700 , loss:  0.007732306\n",
      "Iteration:  19800 , loss:  0.006978384\n",
      "Iteration:  19900 , loss:  0.006985777\n",
      "Generating 7th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.15120327\n",
      "Iteration:  100 , loss:  0.041984428\n",
      "Iteration:  200 , loss:  0.039549753\n",
      "Iteration:  300 , loss:  0.03789808\n",
      "Iteration:  400 , loss:  0.036525283\n",
      "Iteration:  500 , loss:  0.03535919\n",
      "Iteration:  600 , loss:  0.034036294\n",
      "Iteration:  700 , loss:  0.03240437\n",
      "Iteration:  800 , loss:  0.030622931\n",
      "Iteration:  900 , loss:  0.028951444\n",
      "Iteration:  1000 , loss:  0.027304204\n",
      "Iteration:  1100 , loss:  0.025911113\n",
      "Iteration:  1200 , loss:  0.024804981\n",
      "Iteration:  1300 , loss:  0.023862595\n",
      "Iteration:  1400 , loss:  0.022938214\n",
      "Iteration:  1500 , loss:  0.022026746\n",
      "Iteration:  1600 , loss:  0.021155793\n",
      "Iteration:  1700 , loss:  0.020333711\n",
      "Iteration:  1800 , loss:  0.019537117\n",
      "Iteration:  1900 , loss:  0.018721718\n",
      "Iteration:  2000 , loss:  0.017902553\n",
      "Iteration:  2100 , loss:  0.016934264\n",
      "Iteration:  2200 , loss:  0.016046053\n",
      "Iteration:  2300 , loss:  0.015306357\n",
      "Iteration:  2400 , loss:  0.014540199\n",
      "Iteration:  2500 , loss:  0.013845216\n",
      "Iteration:  2600 , loss:  0.013187818\n",
      "Iteration:  2700 , loss:  0.012601884\n",
      "Iteration:  2800 , loss:  0.012134706\n",
      "Iteration:  2900 , loss:  0.011748129\n",
      "Iteration:  3000 , loss:  0.011438255\n",
      "Iteration:  3100 , loss:  0.011125999\n",
      "Iteration:  3200 , loss:  0.010871181\n",
      "Iteration:  3300 , loss:  0.010636503\n",
      "Iteration:  3400 , loss:  0.010432688\n",
      "Iteration:  3500 , loss:  0.010241572\n",
      "Iteration:  3600 , loss:  0.010078356\n",
      "Iteration:  3700 , loss:  0.009921171\n",
      "Iteration:  3800 , loss:  0.0098024765\n",
      "Iteration:  3900 , loss:  0.009655214\n",
      "Iteration:  4000 , loss:  0.009530458\n",
      "Iteration:  4100 , loss:  0.009422703\n",
      "Iteration:  4200 , loss:  0.009315082\n",
      "Iteration:  4300 , loss:  0.009421345\n",
      "Iteration:  4400 , loss:  0.009120788\n",
      "Iteration:  4500 , loss:  0.009025996\n",
      "Iteration:  4600 , loss:  0.008946497\n",
      "Iteration:  4700 , loss:  0.008854568\n",
      "Iteration:  4800 , loss:  0.008781854\n",
      "Iteration:  4900 , loss:  0.008694935\n",
      "Iteration:  5000 , loss:  0.008617934\n",
      "Iteration:  5100 , loss:  0.0085419435\n",
      "Iteration:  5200 , loss:  0.008476484\n",
      "Iteration:  5300 , loss:  0.008410989\n",
      "Iteration:  5400 , loss:  0.008450077\n",
      "Iteration:  5500 , loss:  0.00829996\n",
      "Iteration:  5600 , loss:  0.008249205\n",
      "Iteration:  5700 , loss:  0.008208103\n",
      "Iteration:  5800 , loss:  0.008166964\n",
      "Iteration:  5900 , loss:  0.008133839\n",
      "Iteration:  6000 , loss:  0.008098259\n",
      "Iteration:  6100 , loss:  0.008266541\n",
      "Iteration:  6200 , loss:  0.0080407495\n",
      "Iteration:  6300 , loss:  0.008013425\n",
      "Iteration:  6400 , loss:  0.008033177\n",
      "Iteration:  6500 , loss:  0.00796738\n",
      "Iteration:  6600 , loss:  0.007945835\n",
      "Iteration:  6700 , loss:  0.007927469\n",
      "Iteration:  6800 , loss:  0.007907467\n",
      "Iteration:  6900 , loss:  0.008166368\n",
      "Iteration:  7000 , loss:  0.007873307\n",
      "Iteration:  7100 , loss:  0.007855896\n",
      "Iteration:  7200 , loss:  0.007847054\n",
      "Iteration:  7300 , loss:  0.007826599\n",
      "Iteration:  7400 , loss:  0.007816035\n",
      "Iteration:  7500 , loss:  0.007798963\n",
      "Iteration:  7600 , loss:  0.007784494\n",
      "Iteration:  7700 , loss:  0.00779457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  7800 , loss:  0.007759477\n",
      "Iteration:  7900 , loss:  0.007746189\n",
      "Iteration:  8000 , loss:  0.007743393\n",
      "Iteration:  8100 , loss:  0.0077233757\n",
      "Iteration:  8200 , loss:  0.008005245\n",
      "Iteration:  8300 , loss:  0.0077030053\n",
      "Iteration:  8400 , loss:  0.007692001\n",
      "Iteration:  8500 , loss:  0.0076813144\n",
      "Iteration:  8600 , loss:  0.007673712\n",
      "Iteration:  8700 , loss:  0.007663504\n",
      "Iteration:  8800 , loss:  0.011036554\n",
      "Iteration:  8900 , loss:  0.0076473313\n",
      "Iteration:  9000 , loss:  0.0076386677\n",
      "Iteration:  9100 , loss:  0.007677875\n",
      "Iteration:  9200 , loss:  0.0076242182\n",
      "Iteration:  9300 , loss:  0.0076162103\n",
      "Iteration:  9400 , loss:  0.008070215\n",
      "Iteration:  9500 , loss:  0.0076029003\n",
      "Iteration:  9600 , loss:  0.0075956383\n",
      "Iteration:  9700 , loss:  0.007959357\n",
      "Iteration:  9800 , loss:  0.0075830165\n",
      "Iteration:  9900 , loss:  0.007576041\n",
      "Iteration:  10000 , loss:  0.0075711533\n",
      "Iteration:  10100 , loss:  0.0075644185\n",
      "Iteration:  10200 , loss:  0.008086373\n",
      "Iteration:  10300 , loss:  0.0075530875\n",
      "Iteration:  10400 , loss:  0.007546672\n",
      "Iteration:  10500 , loss:  0.007625696\n",
      "Iteration:  10600 , loss:  0.00753618\n",
      "Iteration:  10700 , loss:  0.007529962\n",
      "Iteration:  10800 , loss:  0.0075717936\n",
      "Iteration:  10900 , loss:  0.007519248\n",
      "Iteration:  11000 , loss:  0.007514035\n",
      "Iteration:  11100 , loss:  0.0075087855\n",
      "Iteration:  11200 , loss:  0.007502591\n",
      "Iteration:  11300 , loss:  0.007730303\n",
      "Iteration:  11400 , loss:  0.0074922834\n",
      "Iteration:  11500 , loss:  0.0074861087\n",
      "Iteration:  11600 , loss:  0.0074968035\n",
      "Iteration:  11700 , loss:  0.007476236\n",
      "Iteration:  11800 , loss:  0.0074700546\n",
      "Iteration:  11900 , loss:  0.0075663337\n",
      "Iteration:  12000 , loss:  0.0074597374\n",
      "Iteration:  12100 , loss:  0.007453393\n",
      "Iteration:  12200 , loss:  0.007457401\n",
      "Iteration:  12300 , loss:  0.00744288\n",
      "Iteration:  12400 , loss:  0.007436403\n",
      "Iteration:  12500 , loss:  0.0074322433\n",
      "Iteration:  12600 , loss:  0.007425596\n",
      "Iteration:  12700 , loss:  0.008477741\n",
      "Iteration:  12800 , loss:  0.007414766\n",
      "Iteration:  12900 , loss:  0.007408315\n",
      "Iteration:  13000 , loss:  0.00741568\n",
      "Iteration:  13100 , loss:  0.0073978654\n",
      "Iteration:  13200 , loss:  0.0073919226\n",
      "Iteration:  13300 , loss:  0.0073877014\n",
      "Iteration:  13400 , loss:  0.0073816036\n",
      "Iteration:  13500 , loss:  0.0074007884\n",
      "Iteration:  13600 , loss:  0.00737203\n",
      "Iteration:  13700 , loss:  0.0073662284\n",
      "Iteration:  13800 , loss:  0.0073631667\n",
      "Iteration:  13900 , loss:  0.0073573915\n",
      "Iteration:  14000 , loss:  0.008738979\n",
      "Iteration:  14100 , loss:  0.007349453\n",
      "Iteration:  14200 , loss:  0.0073441816\n",
      "Iteration:  14300 , loss:  0.0075872494\n",
      "Iteration:  14400 , loss:  0.007336525\n",
      "Iteration:  14500 , loss:  0.0073313657\n",
      "Iteration:  14600 , loss:  0.0073306104\n",
      "Iteration:  14700 , loss:  0.007323671\n",
      "Iteration:  14800 , loss:  0.0073292134\n",
      "Iteration:  14900 , loss:  0.0073163626\n",
      "Iteration:  15000 , loss:  0.0073113088\n",
      "Iteration:  15100 , loss:  0.007325099\n",
      "Iteration:  15200 , loss:  0.0073042675\n",
      "Iteration:  15300 , loss:  0.007299401\n",
      "Iteration:  15400 , loss:  0.0072974255\n",
      "Iteration:  15500 , loss:  0.007292508\n",
      "Iteration:  15600 , loss:  0.0076073264\n",
      "Iteration:  15700 , loss:  0.007285829\n",
      "Iteration:  15800 , loss:  0.0072810454\n",
      "Iteration:  15900 , loss:  0.0072995154\n",
      "Iteration:  16000 , loss:  0.0072743897\n",
      "Iteration:  16100 , loss:  0.007269648\n",
      "Iteration:  16200 , loss:  0.00727049\n",
      "Iteration:  16300 , loss:  0.0072636534\n",
      "Iteration:  16400 , loss:  0.007258994\n",
      "Iteration:  16500 , loss:  0.00726234\n",
      "Iteration:  16600 , loss:  0.0072526922\n",
      "Iteration:  16700 , loss:  0.0072481344\n",
      "Iteration:  16800 , loss:  0.00724617\n",
      "Iteration:  16900 , loss:  0.0072414586\n",
      "Iteration:  17000 , loss:  0.0076515255\n",
      "Iteration:  17100 , loss:  0.0072349263\n",
      "Iteration:  17200 , loss:  0.0072302194\n",
      "Iteration:  17300 , loss:  0.0072452025\n",
      "Iteration:  17400 , loss:  0.007223994\n",
      "Iteration:  17500 , loss:  0.007219342\n",
      "Iteration:  17600 , loss:  0.007233452\n",
      "Iteration:  17700 , loss:  0.0072130775\n",
      "Iteration:  17800 , loss:  0.0072084386\n",
      "Iteration:  17900 , loss:  0.0072102686\n",
      "Iteration:  18000 , loss:  0.0072018835\n",
      "Iteration:  18100 , loss:  0.008827766\n",
      "Iteration:  18200 , loss:  0.0071955477\n",
      "Iteration:  18300 , loss:  0.0071909833\n",
      "Iteration:  18400 , loss:  0.0072581554\n",
      "Iteration:  18500 , loss:  0.0071846256\n",
      "Iteration:  18600 , loss:  0.0071801487\n",
      "Iteration:  18700 , loss:  0.007183162\n",
      "Iteration:  18800 , loss:  0.007173839\n",
      "Iteration:  18900 , loss:  0.009014259\n",
      "Iteration:  19000 , loss:  0.007168264\n",
      "Iteration:  19100 , loss:  0.0071640345\n",
      "Iteration:  19200 , loss:  0.0073237205\n",
      "Iteration:  19300 , loss:  0.0071587274\n",
      "Iteration:  19400 , loss:  0.007154551\n",
      "Iteration:  19500 , loss:  0.0072581493\n",
      "Iteration:  19600 , loss:  0.0071490924\n",
      "Iteration:  19700 , loss:  0.0071452176\n",
      "Iteration:  19800 , loss:  0.0071506514\n",
      "Iteration:  19900 , loss:  0.0071398225\n",
      "Generating 8th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.11111739\n",
      "Iteration:  100 , loss:  0.03996711\n",
      "Iteration:  200 , loss:  0.037420794\n",
      "Iteration:  300 , loss:  0.03525053\n",
      "Iteration:  400 , loss:  0.033407956\n",
      "Iteration:  500 , loss:  0.030846117\n",
      "Iteration:  600 , loss:  0.027928142\n",
      "Iteration:  700 , loss:  0.026185766\n",
      "Iteration:  800 , loss:  0.025006901\n",
      "Iteration:  900 , loss:  0.024062352\n",
      "Iteration:  1000 , loss:  0.022913933\n",
      "Iteration:  1100 , loss:  0.021989964\n",
      "Iteration:  1200 , loss:  0.021046277\n",
      "Iteration:  1300 , loss:  0.02016773\n",
      "Iteration:  1400 , loss:  0.01940791\n",
      "Iteration:  1500 , loss:  0.018695317\n",
      "Iteration:  1600 , loss:  0.017919824\n",
      "Iteration:  1700 , loss:  0.01718611\n",
      "Iteration:  1800 , loss:  0.018477138\n",
      "Iteration:  1900 , loss:  0.01609543\n",
      "Iteration:  2000 , loss:  0.015699942\n",
      "Iteration:  2100 , loss:  0.015328099\n",
      "Iteration:  2200 , loss:  0.014954045\n",
      "Iteration:  2300 , loss:  0.014639462\n",
      "Iteration:  2400 , loss:  0.0143478345\n",
      "Iteration:  2500 , loss:  0.0147986775\n",
      "Iteration:  2600 , loss:  0.013822218\n",
      "Iteration:  2700 , loss:  0.013552501\n",
      "Iteration:  2800 , loss:  0.013292109\n",
      "Iteration:  2900 , loss:  0.0130005665\n",
      "Iteration:  3000 , loss:  0.012893449\n",
      "Iteration:  3100 , loss:  0.012383033\n",
      "Iteration:  3200 , loss:  0.012029694\n",
      "Iteration:  3300 , loss:  0.011668944\n",
      "Iteration:  3400 , loss:  0.011271788\n",
      "Iteration:  3500 , loss:  0.01090371\n",
      "Iteration:  3600 , loss:  0.010553625\n",
      "Iteration:  3700 , loss:  0.010373165\n",
      "Iteration:  3800 , loss:  0.01003334\n",
      "Iteration:  3900 , loss:  0.010693208\n",
      "Iteration:  4000 , loss:  0.009644723\n",
      "Iteration:  4100 , loss:  0.009475008\n",
      "Iteration:  4200 , loss:  0.009339369\n",
      "Iteration:  4300 , loss:  0.009210343\n",
      "Iteration:  4400 , loss:  0.00992023\n",
      "Iteration:  4500 , loss:  0.009006015\n",
      "Iteration:  4600 , loss:  0.0089138355\n",
      "Iteration:  4700 , loss:  0.00884986\n",
      "Iteration:  4800 , loss:  0.008761785\n",
      "Iteration:  4900 , loss:  0.00900273\n",
      "Iteration:  5000 , loss:  0.008626821\n",
      "Iteration:  5100 , loss:  0.008561821\n",
      "Iteration:  5200 , loss:  0.008507747\n",
      "Iteration:  5300 , loss:  0.008450748\n",
      "Iteration:  5400 , loss:  0.008422001\n",
      "Iteration:  5500 , loss:  0.008354718\n",
      "Iteration:  5600 , loss:  0.008365431\n",
      "Iteration:  5700 , loss:  0.008272872\n",
      "Iteration:  5800 , loss:  0.0082341805\n",
      "Iteration:  5900 , loss:  0.008204229\n",
      "Iteration:  6000 , loss:  0.0081703765\n",
      "Iteration:  6100 , loss:  0.008138302\n",
      "Iteration:  6200 , loss:  0.008136409\n",
      "Iteration:  6300 , loss:  0.008084034\n",
      "Iteration:  6400 , loss:  0.008056392\n",
      "Iteration:  6500 , loss:  0.008267821\n",
      "Iteration:  6600 , loss:  0.008006232\n",
      "Iteration:  6700 , loss:  0.007981164\n",
      "Iteration:  6800 , loss:  0.0079602385\n",
      "Iteration:  6900 , loss:  0.007936192\n",
      "Iteration:  7000 , loss:  0.008085649\n",
      "Iteration:  7100 , loss:  0.007893921\n",
      "Iteration:  7200 , loss:  0.007872055\n",
      "Iteration:  7300 , loss:  0.007857742\n",
      "Iteration:  7400 , loss:  0.007834368\n",
      "Iteration:  7500 , loss:  0.007835132\n",
      "Iteration:  7600 , loss:  0.0077983956\n",
      "Iteration:  7700 , loss:  0.0077796197\n",
      "Iteration:  7800 , loss:  0.007799325\n",
      "Iteration:  7900 , loss:  0.007746857\n",
      "Iteration:  8000 , loss:  0.00895888\n",
      "Iteration:  8100 , loss:  0.007716212\n",
      "Iteration:  8200 , loss:  0.0077002705\n",
      "Iteration:  8300 , loss:  0.0076879775\n",
      "Iteration:  8400 , loss:  0.00767302\n",
      "Iteration:  8500 , loss:  0.0077255024\n",
      "Iteration:  8600 , loss:  0.0076475004\n",
      "Iteration:  8700 , loss:  0.007695735\n",
      "Iteration:  8800 , loss:  0.007623821\n",
      "Iteration:  8900 , loss:  0.007611232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  9000 , loss:  0.0076018414\n",
      "Iteration:  9100 , loss:  0.0075901304\n",
      "Iteration:  9200 , loss:  0.008014341\n",
      "Iteration:  9300 , loss:  0.007570659\n",
      "Iteration:  9400 , loss:  0.0075598964\n",
      "Iteration:  9500 , loss:  0.007559271\n",
      "Iteration:  9600 , loss:  0.00754213\n",
      "Iteration:  9700 , loss:  0.007549664\n",
      "Iteration:  9800 , loss:  0.0075251376\n",
      "Iteration:  9900 , loss:  0.007515703\n",
      "Iteration:  10000 , loss:  0.0075110956\n",
      "Iteration:  10100 , loss:  0.0075001456\n",
      "Iteration:  10200 , loss:  0.007932666\n",
      "Iteration:  10300 , loss:  0.0074863983\n",
      "Iteration:  10400 , loss:  0.0074782\n",
      "Iteration:  10500 , loss:  0.008367691\n",
      "Iteration:  10600 , loss:  0.0074651306\n",
      "Iteration:  10700 , loss:  0.007457328\n",
      "Iteration:  10800 , loss:  0.007475604\n",
      "Iteration:  10900 , loss:  0.00744458\n",
      "Iteration:  11000 , loss:  0.0074377977\n",
      "Iteration:  11100 , loss:  0.0074322387\n",
      "Iteration:  11200 , loss:  0.007424931\n",
      "Iteration:  11300 , loss:  0.007444667\n",
      "Iteration:  11400 , loss:  0.007413161\n",
      "Iteration:  11500 , loss:  0.0075878915\n",
      "Iteration:  11600 , loss:  0.007401857\n",
      "Iteration:  11700 , loss:  0.0073950184\n",
      "Iteration:  11800 , loss:  0.007418721\n",
      "Iteration:  11900 , loss:  0.007384194\n",
      "Iteration:  12000 , loss:  0.007377971\n",
      "Iteration:  12100 , loss:  0.0073738494\n",
      "Iteration:  12200 , loss:  0.0073674517\n",
      "Iteration:  12300 , loss:  0.00949931\n",
      "Iteration:  12400 , loss:  0.007358095\n",
      "Iteration:  12500 , loss:  0.0073520015\n",
      "Iteration:  12600 , loss:  0.007346169\n",
      "Iteration:  12700 , loss:  0.0073431446\n",
      "Iteration:  12800 , loss:  0.0073373606\n",
      "Iteration:  12900 , loss:  0.007331559\n",
      "Iteration:  13000 , loss:  0.0073283026\n",
      "Iteration:  13100 , loss:  0.007322644\n",
      "Iteration:  13200 , loss:  0.007369619\n",
      "Iteration:  13300 , loss:  0.0073138284\n",
      "Iteration:  13400 , loss:  0.007308421\n",
      "Iteration:  13500 , loss:  0.007305596\n",
      "Iteration:  13600 , loss:  0.0073003685\n",
      "Iteration:  13700 , loss:  0.0073109204\n",
      "Iteration:  13800 , loss:  0.0072930083\n",
      "Iteration:  13900 , loss:  0.007288023\n",
      "Iteration:  14000 , loss:  0.010123913\n",
      "Iteration:  14100 , loss:  0.0072806138\n",
      "Iteration:  14200 , loss:  0.0072759073\n",
      "Iteration:  14300 , loss:  0.007273573\n",
      "Iteration:  14400 , loss:  0.0072688158\n",
      "Iteration:  14500 , loss:  0.010965653\n",
      "Iteration:  14600 , loss:  0.0072622537\n",
      "Iteration:  14700 , loss:  0.007257905\n",
      "Iteration:  14800 , loss:  0.0072978074\n",
      "Iteration:  14900 , loss:  0.007251906\n",
      "Iteration:  15000 , loss:  0.007247866\n",
      "Iteration:  15100 , loss:  0.007264901\n",
      "Iteration:  15200 , loss:  0.0072417175\n",
      "Iteration:  15300 , loss:  0.00827947\n",
      "Iteration:  15400 , loss:  0.0072364314\n",
      "Iteration:  15500 , loss:  0.0072325636\n",
      "Iteration:  15600 , loss:  0.007781073\n",
      "Iteration:  15700 , loss:  0.0072270064\n",
      "Iteration:  15800 , loss:  0.0072234254\n",
      "Iteration:  15900 , loss:  0.0072284304\n",
      "Iteration:  16000 , loss:  0.0072184014\n",
      "Iteration:  16100 , loss:  0.007214961\n",
      "Iteration:  16200 , loss:  0.0072145313\n",
      "Iteration:  16300 , loss:  0.007210099\n",
      "Iteration:  16400 , loss:  0.007667844\n",
      "Iteration:  16500 , loss:  0.0072051976\n",
      "Iteration:  16600 , loss:  0.0072019445\n",
      "Iteration:  16700 , loss:  0.007200474\n",
      "Iteration:  16800 , loss:  0.0071971957\n",
      "Iteration:  16900 , loss:  0.007252993\n",
      "Iteration:  17000 , loss:  0.007192528\n",
      "Iteration:  17100 , loss:  0.0071903593\n",
      "Iteration:  17200 , loss:  0.0071884296\n",
      "Iteration:  17300 , loss:  0.0071851956\n",
      "Iteration:  17400 , loss:  0.007433157\n",
      "Iteration:  17500 , loss:  0.0071810437\n",
      "Iteration:  17600 , loss:  0.0071782274\n",
      "Iteration:  17700 , loss:  0.0071780193\n",
      "Iteration:  17800 , loss:  0.0071741026\n",
      "Iteration:  17900 , loss:  0.008823495\n",
      "Iteration:  18000 , loss:  0.007170121\n",
      "Iteration:  18100 , loss:  0.0071674297\n",
      "Iteration:  18200 , loss:  0.0072557535\n",
      "Iteration:  18300 , loss:  0.0071635917\n",
      "Iteration:  18400 , loss:  0.007160977\n",
      "Iteration:  18500 , loss:  0.0071633533\n",
      "Iteration:  18600 , loss:  0.007157083\n",
      "Iteration:  18700 , loss:  0.007869537\n",
      "Iteration:  18800 , loss:  0.007153133\n",
      "Iteration:  18900 , loss:  0.007150577\n",
      "Iteration:  19000 , loss:  0.007150049\n",
      "Iteration:  19100 , loss:  0.0071466696\n",
      "Iteration:  19200 , loss:  0.0071993163\n",
      "Iteration:  19300 , loss:  0.007142849\n",
      "Iteration:  19400 , loss:  0.007141441\n",
      "Iteration:  19500 , loss:  0.0071395705\n",
      "Iteration:  19600 , loss:  0.007136669\n",
      "Iteration:  19700 , loss:  0.007140392\n",
      "Iteration:  19800 , loss:  0.007133876\n",
      "Iteration:  19900 , loss:  0.0071314457\n",
      "Generating 9th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.16446424\n",
      "Iteration:  100 , loss:  0.04128787\n",
      "Iteration:  200 , loss:  0.039066616\n",
      "Iteration:  300 , loss:  0.037492007\n",
      "Iteration:  400 , loss:  0.03605412\n",
      "Iteration:  500 , loss:  0.034548674\n",
      "Iteration:  600 , loss:  0.032803863\n",
      "Iteration:  700 , loss:  0.030795228\n",
      "Iteration:  800 , loss:  0.02878351\n",
      "Iteration:  900 , loss:  0.02709291\n",
      "Iteration:  1000 , loss:  0.025738843\n",
      "Iteration:  1100 , loss:  0.024454795\n",
      "Iteration:  1200 , loss:  0.023279814\n",
      "Iteration:  1300 , loss:  0.022330068\n",
      "Iteration:  1400 , loss:  0.021559656\n",
      "Iteration:  1500 , loss:  0.020865375\n",
      "Iteration:  1600 , loss:  0.020185655\n",
      "Iteration:  1700 , loss:  0.019499475\n",
      "Iteration:  1800 , loss:  0.018796561\n",
      "Iteration:  1900 , loss:  0.018083429\n",
      "Iteration:  2000 , loss:  0.017425932\n",
      "Iteration:  2100 , loss:  0.016630895\n",
      "Iteration:  2200 , loss:  0.015876893\n",
      "Iteration:  2300 , loss:  0.015216535\n",
      "Iteration:  2400 , loss:  0.014859341\n",
      "Iteration:  2500 , loss:  0.014951739\n",
      "Iteration:  2600 , loss:  0.01396912\n",
      "Iteration:  2700 , loss:  0.013442681\n",
      "Iteration:  2800 , loss:  0.013035842\n",
      "Iteration:  2900 , loss:  0.012612874\n",
      "Iteration:  3000 , loss:  0.01222815\n",
      "Iteration:  3100 , loss:  0.01350745\n",
      "Iteration:  3200 , loss:  0.011545084\n",
      "Iteration:  3300 , loss:  0.011450892\n",
      "Iteration:  3400 , loss:  0.010958378\n",
      "Iteration:  3500 , loss:  0.010736138\n",
      "Iteration:  3600 , loss:  0.010487064\n",
      "Iteration:  3700 , loss:  0.010284196\n",
      "Iteration:  3800 , loss:  0.0101123825\n",
      "Iteration:  3900 , loss:  0.009944583\n",
      "Iteration:  4000 , loss:  0.009834458\n",
      "Iteration:  4100 , loss:  0.009648466\n",
      "Iteration:  4200 , loss:  0.009526967\n",
      "Iteration:  4300 , loss:  0.00938841\n",
      "Iteration:  4400 , loss:  0.009267975\n",
      "Iteration:  4500 , loss:  0.009166639\n",
      "Iteration:  4600 , loss:  0.009062961\n",
      "Iteration:  4700 , loss:  0.00902752\n",
      "Iteration:  4800 , loss:  0.008894099\n",
      "Iteration:  4900 , loss:  0.008817568\n",
      "Iteration:  5000 , loss:  0.008755085\n",
      "Iteration:  5100 , loss:  0.008692621\n",
      "Iteration:  5200 , loss:  0.009942486\n",
      "Iteration:  5300 , loss:  0.008592169\n",
      "Iteration:  5400 , loss:  0.008545877\n",
      "Iteration:  5500 , loss:  0.008501405\n",
      "Iteration:  5600 , loss:  0.008486989\n",
      "Iteration:  5700 , loss:  0.008423264\n",
      "Iteration:  5800 , loss:  0.008422538\n",
      "Iteration:  5900 , loss:  0.008351502\n",
      "Iteration:  6000 , loss:  0.008314641\n",
      "Iteration:  6100 , loss:  0.008283653\n",
      "Iteration:  6200 , loss:  0.008248702\n",
      "Iteration:  6300 , loss:  0.008218105\n",
      "Iteration:  6400 , loss:  0.008186064\n",
      "Iteration:  6500 , loss:  0.0081550935\n",
      "Iteration:  6600 , loss:  0.00817371\n",
      "Iteration:  6700 , loss:  0.00810073\n",
      "Iteration:  6800 , loss:  0.008073738\n",
      "Iteration:  6900 , loss:  0.008052608\n",
      "Iteration:  7000 , loss:  0.008027845\n",
      "Iteration:  7100 , loss:  0.0082116425\n",
      "Iteration:  7200 , loss:  0.007986533\n",
      "Iteration:  7300 , loss:  0.007965178\n",
      "Iteration:  7400 , loss:  0.00797725\n",
      "Iteration:  7500 , loss:  0.007927818\n",
      "Iteration:  7600 , loss:  0.007908286\n",
      "Iteration:  7700 , loss:  0.007893287\n",
      "Iteration:  7800 , loss:  0.007874476\n",
      "Iteration:  7900 , loss:  0.008014179\n",
      "Iteration:  8000 , loss:  0.007841759\n",
      "Iteration:  8100 , loss:  0.007824615\n",
      "Iteration:  8200 , loss:  0.0078115854\n",
      "Iteration:  8300 , loss:  0.007794772\n",
      "Iteration:  8400 , loss:  0.010086211\n",
      "Iteration:  8500 , loss:  0.00776769\n",
      "Iteration:  8600 , loss:  0.007753242\n",
      "Iteration:  8700 , loss:  0.0077386233\n",
      "Iteration:  8800 , loss:  0.00772808\n",
      "Iteration:  8900 , loss:  0.007714011\n",
      "Iteration:  9000 , loss:  0.007700276\n",
      "Iteration:  9100 , loss:  0.007690946\n",
      "Iteration:  9200 , loss:  0.0076781134\n",
      "Iteration:  9300 , loss:  0.0076654884\n",
      "Iteration:  9400 , loss:  0.0076587307\n",
      "Iteration:  9500 , loss:  0.007644392\n",
      "Iteration:  9600 , loss:  0.007632614\n",
      "Iteration:  9700 , loss:  0.007625512\n",
      "Iteration:  9800 , loss:  0.007613498\n",
      "Iteration:  9900 , loss:  0.007602521\n",
      "Iteration:  10000 , loss:  0.0075968653\n",
      "Iteration:  10100 , loss:  0.0075851525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  10200 , loss:  0.007575083\n",
      "Iteration:  10300 , loss:  0.0075680423\n",
      "Iteration:  10400 , loss:  0.007558303\n",
      "Iteration:  10500 , loss:  0.0075492626\n",
      "Iteration:  10600 , loss:  0.0075419405\n",
      "Iteration:  10700 , loss:  0.007532912\n",
      "Iteration:  10800 , loss:  0.0076423977\n",
      "Iteration:  10900 , loss:  0.007517774\n",
      "Iteration:  11000 , loss:  0.0075092483\n",
      "Iteration:  11100 , loss:  0.0075100134\n",
      "Iteration:  11200 , loss:  0.007494929\n",
      "Iteration:  11300 , loss:  0.009827861\n",
      "Iteration:  11400 , loss:  0.007480944\n",
      "Iteration:  11500 , loss:  0.007473096\n",
      "Iteration:  11600 , loss:  0.0074733775\n",
      "Iteration:  11700 , loss:  0.0074598617\n",
      "Iteration:  11800 , loss:  0.007891473\n",
      "Iteration:  11900 , loss:  0.0074469396\n",
      "Iteration:  12000 , loss:  0.0074396105\n",
      "Iteration:  12100 , loss:  0.0074384646\n",
      "Iteration:  12200 , loss:  0.007427636\n",
      "Iteration:  12300 , loss:  0.0074217133\n",
      "Iteration:  12400 , loss:  0.007416726\n",
      "Iteration:  12500 , loss:  0.0074096867\n",
      "Iteration:  12600 , loss:  0.0074029625\n",
      "Iteration:  12700 , loss:  0.0074017528\n",
      "Iteration:  12800 , loss:  0.007392726\n",
      "Iteration:  12900 , loss:  0.0073863696\n",
      "Iteration:  13000 , loss:  0.00739994\n",
      "Iteration:  13100 , loss:  0.0073759127\n",
      "Iteration:  13200 , loss:  0.007369754\n",
      "Iteration:  13300 , loss:  0.007366032\n",
      "Iteration:  13400 , loss:  0.0073601604\n",
      "Iteration:  13500 , loss:  0.007354798\n",
      "Iteration:  13600 , loss:  0.007351152\n",
      "Iteration:  13700 , loss:  0.0073456233\n",
      "Iteration:  13800 , loss:  0.0073401416\n",
      "Iteration:  13900 , loss:  0.0073385956\n",
      "Iteration:  14000 , loss:  0.0073322337\n",
      "Iteration:  14100 , loss:  0.0073272036\n",
      "Iteration:  14200 , loss:  0.0073339106\n",
      "Iteration:  14300 , loss:  0.007319321\n",
      "Iteration:  14400 , loss:  0.0073144613\n",
      "Iteration:  14500 , loss:  0.0073125\n",
      "Iteration:  14600 , loss:  0.0073076948\n",
      "Iteration:  14700 , loss:  0.0073032384\n",
      "Iteration:  14800 , loss:  0.007305141\n",
      "Iteration:  14900 , loss:  0.0072963135\n",
      "Iteration:  15000 , loss:  0.008366154\n",
      "Iteration:  15100 , loss:  0.0072897812\n",
      "Iteration:  15200 , loss:  0.0072857114\n",
      "Iteration:  15300 , loss:  0.00728957\n",
      "Iteration:  15400 , loss:  0.007279588\n",
      "Iteration:  15500 , loss:  0.0078366045\n",
      "Iteration:  15600 , loss:  0.0072739706\n",
      "Iteration:  15700 , loss:  0.0072702356\n",
      "Iteration:  15800 , loss:  0.007447226\n",
      "Iteration:  15900 , loss:  0.0072649135\n",
      "Iteration:  16000 , loss:  0.0072612986\n",
      "Iteration:  16100 , loss:  0.007264262\n",
      "Iteration:  16200 , loss:  0.0072562215\n",
      "Iteration:  16300 , loss:  0.007252809\n",
      "Iteration:  16400 , loss:  0.007252331\n",
      "Iteration:  16500 , loss:  0.0072483434\n",
      "Iteration:  16600 , loss:  0.007244978\n",
      "Iteration:  16700 , loss:  0.0072441483\n",
      "Iteration:  16800 , loss:  0.0072402987\n",
      "Iteration:  16900 , loss:  0.0075360783\n",
      "Iteration:  17000 , loss:  0.0072356258\n",
      "Iteration:  17100 , loss:  0.0072323955\n",
      "Iteration:  17200 , loss:  0.007232576\n",
      "Iteration:  17300 , loss:  0.0072279223\n",
      "Iteration:  17400 , loss:  0.007332603\n",
      "Iteration:  17500 , loss:  0.007223443\n",
      "Iteration:  17600 , loss:  0.0072203493\n"
     ]
    }
   ],
   "source": [
    "#processes, samples, model = Samplable(x_u_train, t_u_train, u_train, x_f_train, t_f_train, f_train, noise, layers,)\n",
    "\n",
    "processes, samples, model = Trainable(x_u_train, t_u_train, u_train, x_f_train, t_f_train, f_train, noise, layers,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e63fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred, logk_1_pred = model.predict(np.concatenate([x_test, t_test], axis=-1), samples, processes, pde_fn=None,)\n",
    "\n",
    "plots(\n",
    "    logk_1_pred,\n",
    "    u_pred,\n",
    "    x_test,\n",
    "    t_test,\n",
    "    u_test,\n",
    "    x_u_train,\n",
    "    t_u_train,\n",
    "    u_train,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
